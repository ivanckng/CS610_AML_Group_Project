{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz8hKcU_WNSu"
      },
      "source": [
        "### Install and import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG32GA4lWNSy",
        "outputId": "c76e3462-54a8-4004-fd0d-7d070625bdef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in d:\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
            "Requirement already satisfied: torch in d:\\anaconda3\\lib\\site-packages (2.7.0+cu126)\n",
            "Requirement already satisfied: torchvision in d:\\anaconda3\\lib\\site-packages (0.22.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.21.2 in d:\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: filelock in d:\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\anaconda3\\lib\\site-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in d:\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in d:\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in d:\\anaconda3\\lib\\site-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "48nMap0FWNS0"
      },
      "outputs": [],
      "source": [
        "# import cudf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "import tqdm\n",
        "import xgboost as xgb\n",
        "import time\n",
        "from skimage.feature import hog\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "from scipy.stats import randint, uniform\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "import shutil\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVvI74U_WNS1"
      },
      "outputs": [],
      "source": [
        "# Running On CPU, Please skip this cell\n",
        "import cuml\n",
        "print(cuml.__version__)\n",
        "from cuml.model_selection import train_test_split\n",
        "from cuml.metrics import accuracy_score\n",
        "%load_ext cuml.accel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2igsj3fEWNS1"
      },
      "source": [
        "## Check Data Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3QdtbXTWNS2",
        "outputId": "313e3450-3958-4e7c-fefb-a6ac5a0704fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning: datasource\n",
            "Error: Path 'datasource' is not a directory.\n",
            "No images found or path is incorrect/empty.\n"
          ]
        }
      ],
      "source": [
        "def count_images(datasource_path):\n",
        "    image_counts = {}\n",
        "    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'}\n",
        "\n",
        "    if not os.path.isdir(datasource_path):\n",
        "        print(f\"Error: Path '{datasource_path}' is not a directory.\")\n",
        "        return image_counts\n",
        "\n",
        "    for subfolder_name in os.listdir(datasource_path):\n",
        "        subfolder_path = os.path.join(datasource_path, subfolder_name)\n",
        "\n",
        "        if os.path.isdir(subfolder_path):\n",
        "            count = 0\n",
        "            for file_name in os.listdir(subfolder_path):\n",
        "                file_path = os.path.join(subfolder_path, file_name)\n",
        "                if os.path.isfile(file_path):\n",
        "                    _, ext = os.path.splitext(file_name)\n",
        "                    if ext.lower() in image_extensions:\n",
        "                        count += 1\n",
        "            image_counts[subfolder_name] = count\n",
        "    return image_counts\n",
        "\n",
        "image_dir = 'datasource'\n",
        "print(f\"Scanning: {image_dir}\")\n",
        "counts = count_images(image_dir)\n",
        "\n",
        "if counts:\n",
        "    for folder, count in counts.items():\n",
        "        print(f\"{folder}: {count} images\")\n",
        "else:\n",
        "    print(\"No images found or path is incorrect/empty.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3riIuTcgWNS2"
      },
      "source": [
        "## Image Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGHyj-2lWNS3"
      },
      "source": [
        "### Resizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_01yVt1WNS3"
      },
      "outputs": [],
      "source": [
        "# resize images\n",
        "def resize_image_in_folder(input_dir, output_dir, size=(224, 224), desc='resizing images'):\n",
        "    if not os.path.exists(input_dir):\n",
        "        print(f\"Input directory {input_dir} does not exist. Please check the path.\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    supported_formats = ('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp')\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.lower().endswith(supported_formats):\n",
        "            img_input_path = os.path.join(input_dir, filename)\n",
        "            img_output_path = os.path.join(output_dir, filename)\n",
        "            try:\n",
        "                img = cv2.imread(img_input_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "                if img is None:\n",
        "                    print(f\"Error loading {img_input_path}\")\n",
        "                    continue\n",
        "                resized_img = cv2.resize(img, size, interpolation=cv2.INTER_LANCZOS4)\n",
        "\n",
        "                if img_output_path.lower().endswith(('.jpg', '.jpeg')) and resized_img.shape[-1] == 4:\n",
        "                    resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGRA2BGR)\n",
        "                cv2.imwrite(img_output_path, resized_img)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_input_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsayONqzWNS4"
      },
      "outputs": [],
      "source": [
        "# process all folders\n",
        "def batch_resize_images(base_input_dir, base_output_dir, size=(128, 128)):\n",
        "    if not os.path.exists(base_input_dir):\n",
        "        print(f\"Base directory {base_input_dir} does not exist. Please check the path.\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(base_output_dir, exist_ok=True) # if output directory does not exist, create it.\n",
        "\n",
        "    for folder in tqdm.tqdm(os.listdir(base_input_dir)):\n",
        "        current_input_subfolder = os.path.join(base_input_dir, folder)\n",
        "        current_output_subfolder = os.path.join(base_output_dir, folder)\n",
        "\n",
        "        if os.path.isdir(current_input_subfolder):\n",
        "            resize_image_in_folder(current_input_subfolder, current_output_subfolder, size=size)\n",
        "        else:\n",
        "            print(f\"Skipping {current_input_subfolder} as it is not a directory.\")\n",
        "\n",
        "    print(\"Batch resizing completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUovyvUgWNS4",
        "outputId": "fcbd13bb-8a1c-43ba-d6c3-30f012371559"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:47<00:00,  2.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch resizing completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "input_dir = '../CS610_AML_Group_Project/datasource'\n",
        "output_dir = '../CS610_AML_Group_Project/resized_images'\n",
        "batch_resize_images(input_dir, output_dir, size=(128, 128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpIJ4twGWNS4"
      },
      "source": [
        "### Gray Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18eO3kYtWNS4"
      },
      "outputs": [],
      "source": [
        "def grayscale_image_in_folder(input_dir, output_dir):\n",
        "    if not os.path.exists(input_dir):\n",
        "        print(f\"Input directory {input_dir} does not exist. Please check the path.\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    supported_formats = ('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp')\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.lower().endswith(supported_formats):\n",
        "            img_input_path = os.path.join(input_dir, filename)\n",
        "            img_output_path = os.path.join(output_dir, filename)\n",
        "            try:\n",
        "                img = cv2.imread(img_input_path)\n",
        "                if img is None:\n",
        "                    print(f\"Error loading {img_input_path}\")\n",
        "                    continue\n",
        "                # Convert to grayscale\n",
        "                gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                cv2.imwrite(img_output_path, gray_img)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_input_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC1GDqBiWNS5"
      },
      "outputs": [],
      "source": [
        "def batch_grayscale_images(base_input_dir, base_output_dir):\n",
        "    if not os.path.exists(base_input_dir):\n",
        "        print(f\"Base directory {base_input_dir} does not exist. Please check the path.\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(base_output_dir, exist_ok=True)\n",
        "\n",
        "    for folder in tqdm.tqdm(os.listdir(base_input_dir)):\n",
        "        current_input_subfolder = os.path.join(base_input_dir, folder)\n",
        "        current_output_subfolder = os.path.join(base_output_dir, folder)\n",
        "\n",
        "        if os.path.isdir(current_input_subfolder):\n",
        "            grayscale_image_in_folder(current_input_subfolder, current_output_subfolder)\n",
        "        else:\n",
        "            print(f\"Skipping {current_input_subfolder} as it is not a directory.\")\n",
        "\n",
        "    print(\"Batch grayscale completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwQldSOfWNS5",
        "outputId": "9c528463-62a8-488e-adf8-c1943d2e6daf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:29<00:00,  1.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch grayscale completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "base_input_dir = '../CS610_AML_Group_Project/resized_images'\n",
        "base_output_dir = '../CS610_AML_Group_Project/grayscale_images'\n",
        "batch_grayscale_images(base_input_dir, base_output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEfnyf6sWNS5"
      },
      "outputs": [],
      "source": [
        "def image_to_df(data_dir):\n",
        "    \"\"\"\n",
        "    Create a DataFrame with image paths and their corresponding class labels\n",
        "    \"\"\"\n",
        "    image_data = []\n",
        "\n",
        "    # Walk through all subdirectories in the data directory\n",
        "    for class_name in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "\n",
        "        # Skip if not a directory\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        # Get all image files in the class directory\n",
        "        for filename in os.listdir(class_path):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                image_path = os.path.join(class_path, filename)\n",
        "                image_data.append({\n",
        "                    'image_path': image_path,\n",
        "                    'class': class_name\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(image_data)\n",
        "\n",
        "def split_dataset(data_dir, output_dir, test_size=0.2, val_size=0.1, random_state=42):\n",
        "    \"\"\"\n",
        "    Split the dataset into training, validation, and test sets\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing the original dataset\n",
        "        output_dir: Directory to save the split datasets\n",
        "        test_size: Proportion of data for test set (default: 0.2)\n",
        "        val_size: Proportion of remaining data for validation set (default: 0.2)\n",
        "        random_state: Random seed for reproducibility\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Creating dataset DataFrame...\")\n",
        "    df = image_to_df(data_dir)\n",
        "\n",
        "    print(f\"Total images found: {len(df)}\")\n",
        "    print(f\"Number of classes: {df['class'].nunique()}\")\n",
        "    print(\"\\nClass distribution:\")\n",
        "    print(df['class'].value_counts())\n",
        "\n",
        "    # Encode class labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    df['encoded_class'] = label_encoder.fit_transform(df['class'])\n",
        "\n",
        "    # Display class mapping\n",
        "    print(\"\\nClass to encoded label mapping:\")\n",
        "    for i, class_name in enumerate(label_encoder.classes_):\n",
        "        print(f\"{class_name}: {i}\")\n",
        "\n",
        "    # First split: separate test set\n",
        "    train_df, test_df = train_test_split(\n",
        "        df,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        stratify=df['encoded_class']\n",
        "    )\n",
        "\n",
        "    # Create output directories\n",
        "    train_dir = os.path.join(output_dir, 'train')\n",
        "    test_dir = os.path.join(output_dir, 'test')\n",
        "\n",
        "    for dir_path in [train_dir, test_dir]:\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "    # Copy files to respective directories\n",
        "    print(\"\\nCopying files to split directories...\")\n",
        "\n",
        "    # Copy training files\n",
        "    print(\"Copying training files...\")\n",
        "    for _, row in tqdm.tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "        class_dir = os.path.join(train_dir, row['class'])\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "        filename = os.path.basename(row['image_path'])\n",
        "        dest_path = os.path.join(class_dir, filename)\n",
        "        shutil.copy2(row['image_path'], dest_path)\n",
        "\n",
        "    # Copy test files\n",
        "    print(\"Copying test files...\")\n",
        "    for _, row in tqdm.tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "        class_dir = os.path.join(test_dir, row['class'])\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "        filename = os.path.basename(row['image_path'])\n",
        "        dest_path = os.path.join(class_dir, filename)\n",
        "        shutil.copy2(row['image_path'], dest_path)\n",
        "\n",
        "    # Save split information\n",
        "    split_info = {\n",
        "        'total_images': len(df),\n",
        "        'train_images': len(train_df),\n",
        "        'test_images': len(test_df),\n",
        "        'num_classes': len(label_encoder.classes_),\n",
        "        'class_mapping': dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
        "    }\n",
        "\n",
        "    # Save DataFrames\n",
        "    train_df.to_csv(os.path.join(output_dir, 'train_split.csv'), index=False)\n",
        "    test_df.to_csv(os.path.join(output_dir, 'test_split.csv'), index=False)\n",
        "\n",
        "    # Save split information\n",
        "    split_info_df = pd.DataFrame([split_info])\n",
        "    split_info_df.to_csv(os.path.join(output_dir, 'split_info.csv'), index=False)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DATASET SPLIT SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Total images: {len(df)}\")\n",
        "    print(f\"Training set: {len(train_df)} images ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "    print(f\"Test set: {len(test_df)} images ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "    print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
        "    print(f\"\\nSplit datasets saved to: {output_dir}\")\n",
        "\n",
        "    return train_df, test_df, label_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha3tBZK3WNS6",
        "outputId": "b84b31db-3449-478e-d855-c8eb6eb8c918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating dataset DataFrame...\n",
            "Total images found: 6480\n",
            "Number of classes: 50\n",
            "\n",
            "Class distribution:\n",
            "class\n",
            "adidas_forum_high                      150\n",
            "nike_air_jordan_4                      150\n",
            "nike_air_max_90                        150\n",
            "new_balance_992                        150\n",
            "new_balance_574                        150\n",
            "new_balance_550                        150\n",
            "adidas_ultraboost                      150\n",
            "nike_cortez                            150\n",
            "converse_one_star                      150\n",
            "nike_dunk_high                         150\n",
            "nike_air_vapormax_flyknit              149\n",
            "nike_air_max_270                       149\n",
            "vans_sk8-hi                            149\n",
            "adidas_gazelle                         149\n",
            "converse_chuck_70_low                  148\n",
            "reebok_club_c_85                       148\n",
            "vans_authentic                         148\n",
            "yeezy_boost_350_v2                     148\n",
            "nike_air_force_1_mid                   148\n",
            "puma_suede_classic                     148\n",
            "salomon_xt-6                           147\n",
            "adidas_stan_smith                      147\n",
            "nike_air_force_1_low                   147\n",
            "yeezy_slide                            145\n",
            "nike_air_jordan_1_low                  115\n",
            "adidas_nmd_r1                          115\n",
            "vans_slip-on_checkerboard              115\n",
            "adidas_samba                           115\n",
            "nike_air_force_1_high                  115\n",
            "nike_air_max_95                        115\n",
            "nike_air_max_97                        115\n",
            "nike_air_max_plus_(tn)                 115\n",
            "converse_chuck_70_high                 115\n",
            "nike_blazer_mid_77                     115\n",
            "reebok_classic_leather                 115\n",
            "adidas_forum_low                       115\n",
            "nike_dunk_low                          115\n",
            "converse_chuck_taylor_all-star_high    114\n",
            "adidas_superstar                       114\n",
            "converse_chuck_taylor_all-star_low     114\n",
            "nike_air_jordan_1_high                 114\n",
            "new_balance_990                        113\n",
            "vans_old_skool                         113\n",
            "nike_air_jordan_11                     113\n",
            "new_balance_327                        108\n",
            "yeezy_700_wave_runner                  108\n",
            "nike_air_vapormax_plus                 107\n",
            "nike_air_max_1                         106\n",
            "nike_air_jordan_3                      100\n",
            "asics_gel-lyte_iii                      91\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class to encoded label mapping:\n",
            "adidas_forum_high: 0\n",
            "adidas_forum_low: 1\n",
            "adidas_gazelle: 2\n",
            "adidas_nmd_r1: 3\n",
            "adidas_samba: 4\n",
            "adidas_stan_smith: 5\n",
            "adidas_superstar: 6\n",
            "adidas_ultraboost: 7\n",
            "asics_gel-lyte_iii: 8\n",
            "converse_chuck_70_high: 9\n",
            "converse_chuck_70_low: 10\n",
            "converse_chuck_taylor_all-star_high: 11\n",
            "converse_chuck_taylor_all-star_low: 12\n",
            "converse_one_star: 13\n",
            "new_balance_327: 14\n",
            "new_balance_550: 15\n",
            "new_balance_574: 16\n",
            "new_balance_990: 17\n",
            "new_balance_992: 18\n",
            "nike_air_force_1_high: 19\n",
            "nike_air_force_1_low: 20\n",
            "nike_air_force_1_mid: 21\n",
            "nike_air_jordan_11: 22\n",
            "nike_air_jordan_1_high: 23\n",
            "nike_air_jordan_1_low: 24\n",
            "nike_air_jordan_3: 25\n",
            "nike_air_jordan_4: 26\n",
            "nike_air_max_1: 27\n",
            "nike_air_max_270: 28\n",
            "nike_air_max_90: 29\n",
            "nike_air_max_95: 30\n",
            "nike_air_max_97: 31\n",
            "nike_air_max_plus_(tn): 32\n",
            "nike_air_vapormax_flyknit: 33\n",
            "nike_air_vapormax_plus: 34\n",
            "nike_blazer_mid_77: 35\n",
            "nike_cortez: 36\n",
            "nike_dunk_high: 37\n",
            "nike_dunk_low: 38\n",
            "puma_suede_classic: 39\n",
            "reebok_classic_leather: 40\n",
            "reebok_club_c_85: 41\n",
            "salomon_xt-6: 42\n",
            "vans_authentic: 43\n",
            "vans_old_skool: 44\n",
            "vans_sk8-hi: 45\n",
            "vans_slip-on_checkerboard: 46\n",
            "yeezy_700_wave_runner: 47\n",
            "yeezy_boost_350_v2: 48\n",
            "yeezy_slide: 49\n",
            "\n",
            "Copying files to split directories...\n",
            "Copying training files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5184/5184 [00:03<00:00, 1614.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying test files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1296/1296 [00:00<00:00, 1657.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "DATASET SPLIT SUMMARY\n",
            "==================================================\n",
            "Total images: 6480\n",
            "Training set: 5184 images (80.0%)\n",
            "Test set: 1296 images (20.0%)\n",
            "Number of classes: 50\n",
            "\n",
            "Split datasets saved to: ../CS610_AML_Group_Project/split_images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(                                             image_path  \\\n",
              " 6177  ../CS610_AML_Group_Project/grayscale_images\\ye...   \n",
              " 288   ../CS610_AML_Group_Project/grayscale_images\\ad...   \n",
              " 6340  ../CS610_AML_Group_Project/grayscale_images\\ye...   \n",
              " 3096  ../CS610_AML_Group_Project/grayscale_images\\ni...   \n",
              " 3640  ../CS610_AML_Group_Project/grayscale_images\\ni...   \n",
              " ...                                                 ...   \n",
              " 2769  ../CS610_AML_Group_Project/grayscale_images\\ni...   \n",
              " 5597  ../CS610_AML_Group_Project/grayscale_images\\va...   \n",
              " 5385  ../CS610_AML_Group_Project/grayscale_images\\re...   \n",
              " 4353  ../CS610_AML_Group_Project/grayscale_images\\ni...   \n",
              " 1725  ../CS610_AML_Group_Project/grayscale_images\\co...   \n",
              " \n",
              "                           class  encoded_class  \n",
              " 6177      yeezy_700_wave_runner             47  \n",
              " 288              adidas_gazelle              2  \n",
              " 6340                yeezy_slide             49  \n",
              " 3096      nike_air_jordan_1_low             24  \n",
              " 3640           nike_air_max_270             28  \n",
              " ...                         ...            ...  \n",
              " 2769       nike_air_force_1_mid             21  \n",
              " 5597             vans_authentic             43  \n",
              " 5385           reebok_club_c_85             41  \n",
              " 4353  nike_air_vapormax_flyknit             33  \n",
              " 1725          converse_one_star             13  \n",
              " \n",
              " [5184 rows x 3 columns],\n",
              "                                              image_path  \\\n",
              " 4337  ../CS610_AML_Group_Project/grayscale_images\\ni...   \n",
              " 2029  ../CS610_AML_Group_Project/grayscale_images\\ne...   \n",
              " 5465  ../CS610_AML_Group_Project/grayscale_images\\sa...   \n",
              " 541   ../CS610_AML_Group_Project/grayscale_images\\ad...   \n",
              " 6358  ../CS610_AML_Group_Project/grayscale_images\\ye...   \n",
              " ...                                                 ...   \n",
              " 5090  ../CS610_AML_Group_Project/grayscale_images\\pu...   \n",
              " 5292  ../CS610_AML_Group_Project/grayscale_images\\re...   \n",
              " 1233  ../CS610_AML_Group_Project/grayscale_images\\co...   \n",
              " 3977  ../CS610_AML_Group_Project/grayscale_images\\ni...   \n",
              " 1706  ../CS610_AML_Group_Project/grayscale_images\\co...   \n",
              " \n",
              "                           class  encoded_class  \n",
              " 4337  nike_air_vapormax_flyknit             33  \n",
              " 2029            new_balance_550             15  \n",
              " 5465               salomon_xt-6             42  \n",
              " 541                adidas_samba              4  \n",
              " 6358                yeezy_slide             49  \n",
              " ...                         ...            ...  \n",
              " 5090         puma_suede_classic             39  \n",
              " 5292           reebok_club_c_85             41  \n",
              " 1233     converse_chuck_70_high              9  \n",
              " 3977            nike_air_max_95             30  \n",
              " 1706          converse_one_star             13  \n",
              " \n",
              " [1296 rows x 3 columns],\n",
              " LabelEncoder())"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_dir = '../CS610_AML_Group_Project/grayscale_images'\n",
        "out_dir = '../CS610_AML_Group_Project/split_images'\n",
        "split_dataset(data_dir, out_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3_pR9iFWNS6"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGoXRcNQWNS6"
      },
      "outputs": [],
      "source": [
        "def image_augmentation(image, augmentation_type,angle_range=(-15, 15), brightness_range=(0.7, 1.3)):\n",
        "    if augmentation_type == 'flip':\n",
        "        return cv2.flip(image, 1)\n",
        "    elif augmentation_type == 'rotate':\n",
        "        angle = np.random.randint(angle_range[0], angle_range[1] + 1)\n",
        "        (h, w) = image.shape[:2]\n",
        "        center = (w // 2, h // 2)\n",
        "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "        return cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
        "    elif augmentation_type == 'brightness':\n",
        "        brightness_factor = np.random.uniform(brightness_range[0], brightness_range[1])\n",
        "        return np.clip(image * brightness_factor, 0, 255).astype(np.uint8)\n",
        "    return image # return orginal image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6zJj3hSWNS6",
        "outputId": "a1fd4bed-1b06-46b5-e053-c0f36d06bf52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== Image Augmentation Starts ======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Augmenting: 100%|██████████| 50/50 [01:16<00:00,  1.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== Image Augmentation Starts ======\n",
            "We have now 20736 images for modelling\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>augmented_train_images\\adidas_forum_high\\0001_...</td>\n",
              "      <td>adidas_forum_high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>augmented_train_images\\adidas_forum_high\\0001_...</td>\n",
              "      <td>adidas_forum_high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>augmented_train_images\\adidas_forum_high\\0001_...</td>\n",
              "      <td>adidas_forum_high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>augmented_train_images\\adidas_forum_high\\0001_...</td>\n",
              "      <td>adidas_forum_high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>augmented_train_images\\adidas_forum_high\\0004_...</td>\n",
              "      <td>adidas_forum_high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20731</th>\n",
              "      <td>augmented_train_images\\yeezy_slide\\0144_bright...</td>\n",
              "      <td>yeezy_slide</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20732</th>\n",
              "      <td>augmented_train_images\\yeezy_slide\\0145_origin...</td>\n",
              "      <td>yeezy_slide</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20733</th>\n",
              "      <td>augmented_train_images\\yeezy_slide\\0145_flippe...</td>\n",
              "      <td>yeezy_slide</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20734</th>\n",
              "      <td>augmented_train_images\\yeezy_slide\\0145_rotate...</td>\n",
              "      <td>yeezy_slide</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20735</th>\n",
              "      <td>augmented_train_images\\yeezy_slide\\0145_bright...</td>\n",
              "      <td>yeezy_slide</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20736 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    path              label\n",
              "0      augmented_train_images\\adidas_forum_high\\0001_...  adidas_forum_high\n",
              "1      augmented_train_images\\adidas_forum_high\\0001_...  adidas_forum_high\n",
              "2      augmented_train_images\\adidas_forum_high\\0001_...  adidas_forum_high\n",
              "3      augmented_train_images\\adidas_forum_high\\0001_...  adidas_forum_high\n",
              "4      augmented_train_images\\adidas_forum_high\\0004_...  adidas_forum_high\n",
              "...                                                  ...                ...\n",
              "20731  augmented_train_images\\yeezy_slide\\0144_bright...        yeezy_slide\n",
              "20732  augmented_train_images\\yeezy_slide\\0145_origin...        yeezy_slide\n",
              "20733  augmented_train_images\\yeezy_slide\\0145_flippe...        yeezy_slide\n",
              "20734  augmented_train_images\\yeezy_slide\\0145_rotate...        yeezy_slide\n",
              "20735  augmented_train_images\\yeezy_slide\\0145_bright...        yeezy_slide\n",
              "\n",
              "[20736 rows x 2 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a directory to store the augmented images\n",
        "aug_dir = 'augmented_train_images'\n",
        "os.makedirs(aug_dir, exist_ok = True)\n",
        "image_dir = '../CS610_AML_Group_Project/split_images/train'\n",
        "\n",
        "all_images_paths = []\n",
        "all_images_labels = []\n",
        "\n",
        "\n",
        "\n",
        "sneaker_names_list = os.listdir(image_dir)\n",
        "print(\"====== Image Augmentation Starts ======\")\n",
        "for sneaker_name in tqdm.tqdm(sneaker_names_list, desc=\"Augmenting\"):\n",
        "    original_path = os.path.join(image_dir, sneaker_name)\n",
        "    if os.path.isdir(original_path):\n",
        "        aug_path = os.path.join(aug_dir, sneaker_name)\n",
        "        os.makedirs(aug_path, exist_ok = True)\n",
        "\n",
        "        for image in os.listdir(original_path):\n",
        "\n",
        "            # design saved path\n",
        "            # 1. orinal\n",
        "            image_full_path = os.path.join(original_path, image)\n",
        "            original_image = cv2.imread(image_full_path)\n",
        "            if original_image is None:\n",
        "                print(f'WARNING: CANNOT READ IMAGE {image_full_path}, SKIPPED!')\n",
        "                continue\n",
        "\n",
        "            base, ext = os.path.splitext(image)\n",
        "\n",
        "            # design saved path\n",
        "            # 1. orinal\n",
        "            image_name_original = f'{base}_original{ext}'\n",
        "            original_image_saved_path = os.path.join(aug_path,image_name_original)\n",
        "            # 2. flipped\n",
        "            image_name_flipped = f'{base}_flipped{ext}'\n",
        "            flipped_image_saved_path = os.path.join(aug_path,image_name_flipped)\n",
        "            # 3. rotated\n",
        "            image_name_rotated = f'{base}_rotated{ext}'\n",
        "            rotated_image_saved_path = os.path.join(aug_path,image_name_rotated)\n",
        "            # 4. bright\n",
        "            image_name_brightened = f'{base}_brightened{ext}'\n",
        "            brightened_image_saved_path = os.path.join(aug_path,image_name_brightened)\n",
        "\n",
        "            # augmentation operations\n",
        "            # 1. original\n",
        "            cv2.imwrite(original_image_saved_path, original_image)\n",
        "            all_images_paths.append(original_image_saved_path)\n",
        "            all_images_labels.append(sneaker_name)\n",
        "\n",
        "            # 2. flipped\n",
        "            img_flipped = image_augmentation(original_image, augmentation_type = 'flip')\n",
        "            cv2.imwrite(flipped_image_saved_path, img_flipped)\n",
        "            all_images_paths.append(flipped_image_saved_path)\n",
        "            all_images_labels.append(sneaker_name)\n",
        "\n",
        "            # 3. rotated\n",
        "            img_rotated = image_augmentation(original_image, augmentation_type = 'rotate')\n",
        "            cv2.imwrite(rotated_image_saved_path, img_rotated)\n",
        "            all_images_paths.append(rotated_image_saved_path)\n",
        "            all_images_labels.append(sneaker_name)\n",
        "\n",
        "            # 4. brightness\n",
        "            img_bright = image_augmentation(original_image, augmentation_type = 'brightness')\n",
        "            cv2.imwrite(brightened_image_saved_path, img_bright)\n",
        "            all_images_paths.append(brightened_image_saved_path)\n",
        "            all_images_labels.append(sneaker_name)\n",
        "\n",
        "print(\"====== Image Augmentation Starts ======\")\n",
        "\n",
        "image_df_augmented = pd.DataFrame({\n",
        "    'path': all_images_paths,\n",
        "    'label': all_images_labels\n",
        "})\n",
        "\n",
        "print(f\"We have now {len(image_df_augmented)} images for modelling\")\n",
        "\n",
        "image_df_augmented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95a1IgpIWNS7"
      },
      "source": [
        "### Pipeline Models using Feature Extraction Method 1 - By HOG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pothiefWNS7"
      },
      "source": [
        "#### Feature Extraction by HOG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rsCoe8TrWNS7"
      },
      "outputs": [],
      "source": [
        "def extract_hog_features_recursive(input_dir, force_size = (128, 128), pixels_per_cell=(16, 16), cells_per_block=(2, 2)):\n",
        "    features = []\n",
        "    filenames = []\n",
        "    supported_formats = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.webp')\n",
        "    for root, dirs, files in tqdm.tqdm(os.walk(input_dir)):\n",
        "        for filename in tqdm.tqdm(files):\n",
        "            if filename.lower().endswith(supported_formats):\n",
        "                img_path = os.path.join(root, filename)\n",
        "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "                if img is None:\n",
        "                    continue\n",
        "                # force resized in case feature extraction failed\n",
        "                img_resized = cv2.resize(img, force_size, interpolation=cv2.INTER_AREA)\n",
        "                # pixel normalisation\n",
        "                img_normalised = img_resized.astype(np.float32) / 255.0\n",
        "                # Extract HOG features\n",
        "                try:\n",
        "                    hog_feature = hog(img_normalised, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block, feature_vector=True)\n",
        "                    features.append(hog_feature)\n",
        "                    rel_path = os.path.relpath(img_path, input_dir)\n",
        "                    filenames.append(rel_path)\n",
        "                except Exception as e:\n",
        "                    print(\"WARNING: {img_path} Failed with HOG feature extraction!\")\n",
        "                    continue\n",
        "    hogged = np.array(features)\n",
        "    return hogged, filenames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LLmtS7vCWNS7",
        "outputId": "bd9c9889-47dc-442b-ca9b-43f638ab7a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== HOG Extraction Starts! ======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "  0%|          | 0/472 [00:00<?, ?it/s]\u001b[A\n",
            "  5%|▍         | 22/472 [00:00<00:02, 216.46it/s]\u001b[A\n",
            " 12%|█▎        | 59/472 [00:00<00:01, 302.83it/s]\u001b[A\n",
            " 20%|██        | 96/472 [00:00<00:01, 332.16it/s]\u001b[A\n",
            " 28%|██▊       | 133/472 [00:00<00:00, 343.91it/s]\u001b[A\n",
            " 36%|███▌      | 170/472 [00:00<00:00, 350.87it/s]\u001b[A\n",
            " 44%|████▍     | 207/472 [00:00<00:00, 356.44it/s]\u001b[A\n",
            " 51%|█████▏    | 243/472 [00:00<00:00, 355.70it/s]\u001b[A\n",
            " 60%|█████▉    | 281/472 [00:00<00:00, 360.83it/s]\u001b[A\n",
            " 67%|██████▋   | 318/472 [00:00<00:00, 363.14it/s]\u001b[A\n",
            " 75%|███████▌  | 355/472 [00:01<00:00, 363.30it/s]\u001b[A\n",
            " 83%|████████▎ | 392/472 [00:01<00:00, 364.07it/s]\u001b[A\n",
            " 91%|█████████ | 430/472 [00:01<00:00, 365.81it/s]\u001b[A\n",
            "100%|██████████| 472/472 [00:01<00:00, 353.22it/s]\n",
            "2it [00:01,  1.49it/s]\n",
            "  0%|          | 0/476 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 38/476 [00:00<00:01, 378.03it/s]\u001b[A\n",
            " 16%|█▌        | 76/476 [00:00<00:01, 379.04it/s]\u001b[A\n",
            " 24%|██▍       | 115/476 [00:00<00:00, 380.13it/s]\u001b[A\n",
            " 32%|███▏      | 154/476 [00:00<00:00, 379.68it/s]\u001b[A\n",
            " 40%|████      | 192/476 [00:00<00:00, 379.52it/s]\u001b[A\n",
            " 48%|████▊     | 230/476 [00:00<00:00, 374.07it/s]\u001b[A\n",
            " 57%|█████▋    | 269/476 [00:00<00:00, 376.54it/s]\u001b[A\n",
            " 64%|██████▍   | 307/476 [00:00<00:00, 376.95it/s]\u001b[A\n",
            " 72%|███████▏  | 345/476 [00:00<00:00, 377.07it/s]\u001b[A\n",
            " 81%|████████  | 385/476 [00:01<00:00, 383.06it/s]\u001b[A\n",
            " 89%|████████▉ | 424/476 [00:01<00:00, 382.79it/s]\u001b[A\n",
            "100%|██████████| 476/476 [00:01<00:00, 378.34it/s]\n",
            "3it [00:02,  1.09it/s]\n",
            "  0%|          | 0/472 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 40/472 [00:00<00:01, 399.03it/s]\u001b[A\n",
            " 17%|█▋        | 80/472 [00:00<00:01, 391.39it/s]\u001b[A\n",
            " 25%|██▌       | 120/472 [00:00<00:00, 394.63it/s]\u001b[A\n",
            " 34%|███▍      | 160/472 [00:00<00:00, 393.13it/s]\u001b[A\n",
            " 42%|████▏     | 200/472 [00:00<00:00, 392.90it/s]\u001b[A\n",
            " 51%|█████     | 240/472 [00:00<00:00, 394.31it/s]\u001b[A\n",
            " 59%|█████▉    | 280/472 [00:00<00:00, 377.71it/s]\u001b[A\n",
            " 68%|██████▊   | 320/472 [00:00<00:00, 383.51it/s]\u001b[A\n",
            " 76%|███████▌  | 359/472 [00:00<00:00, 384.54it/s]\u001b[A\n",
            " 84%|████████▍ | 398/472 [00:01<00:00, 385.92it/s]\u001b[A\n",
            "100%|██████████| 472/472 [00:01<00:00, 387.22it/s]\n",
            "4it [00:03,  1.03s/it]\n",
            "  0%|          | 0/368 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 37/368 [00:00<00:00, 367.61it/s]\u001b[A\n",
            " 20%|██        | 75/368 [00:00<00:00, 371.96it/s]\u001b[A\n",
            " 31%|███       | 113/368 [00:00<00:00, 373.38it/s]\u001b[A\n",
            " 42%|████▏     | 153/368 [00:00<00:00, 380.48it/s]\u001b[A\n",
            " 52%|█████▏    | 192/368 [00:00<00:00, 383.26it/s]\u001b[A\n",
            " 63%|██████▎   | 232/368 [00:00<00:00, 387.40it/s]\u001b[A\n",
            " 74%|███████▍  | 272/368 [00:00<00:00, 389.54it/s]\u001b[A\n",
            " 85%|████████▍ | 311/368 [00:00<00:00, 388.69it/s]\u001b[A\n",
            "100%|██████████| 368/368 [00:00<00:00, 385.02it/s]\n",
            "5it [00:04,  1.01s/it]\n",
            "  0%|          | 0/472 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 35/472 [00:00<00:01, 347.37it/s]\u001b[A\n",
            " 16%|█▌        | 74/472 [00:00<00:01, 369.74it/s]\u001b[A\n",
            " 24%|██▍       | 115/472 [00:00<00:00, 383.49it/s]\u001b[A\n",
            " 33%|███▎      | 154/472 [00:00<00:00, 379.55it/s]\u001b[A\n",
            " 41%|████      | 193/472 [00:00<00:00, 382.30it/s]\u001b[A\n",
            " 49%|████▉     | 232/472 [00:00<00:00, 380.30it/s]\u001b[A\n",
            " 57%|█████▋    | 271/472 [00:00<00:00, 378.78it/s]\u001b[A\n",
            " 66%|██████▌   | 311/472 [00:00<00:00, 383.85it/s]\u001b[A\n",
            " 74%|███████▍  | 350/472 [00:00<00:00, 382.82it/s]\u001b[A\n",
            " 83%|████████▎ | 390/472 [00:01<00:00, 386.18it/s]\u001b[A\n",
            " 91%|█████████ | 429/472 [00:01<00:00, 385.88it/s]\u001b[A\n",
            "100%|██████████| 472/472 [00:01<00:00, 380.14it/s]\n",
            "6it [00:06,  1.09s/it]\n",
            "  0%|          | 0/472 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 38/472 [00:00<00:01, 377.28it/s]\u001b[A\n",
            " 16%|█▌        | 76/472 [00:00<00:01, 376.08it/s]\u001b[A\n",
            " 24%|██▍       | 114/472 [00:00<00:00, 377.07it/s]\u001b[A\n",
            " 32%|███▏      | 152/472 [00:00<00:00, 372.24it/s]\u001b[A\n",
            " 40%|████      | 190/472 [00:00<00:00, 368.74it/s]\u001b[A\n",
            " 48%|████▊     | 227/472 [00:00<00:00, 367.11it/s]\u001b[A\n",
            " 56%|█████▌    | 264/472 [00:00<00:00, 366.29it/s]\u001b[A\n",
            " 64%|██████▍   | 302/472 [00:00<00:00, 367.45it/s]\u001b[A\n",
            " 72%|███████▏  | 339/472 [00:00<00:00, 363.41it/s]\u001b[A\n",
            " 80%|███████▉  | 377/472 [00:01<00:00, 367.12it/s]\u001b[A\n",
            " 88%|████████▊ | 414/472 [00:01<00:00, 362.33it/s]\u001b[A\n",
            "100%|██████████| 472/472 [00:01<00:00, 367.26it/s]\n",
            "7it [00:07,  1.15s/it]\n",
            "  0%|          | 0/348 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 36/348 [00:00<00:00, 358.28it/s]\u001b[A\n",
            " 21%|██        | 72/348 [00:00<00:00, 355.44it/s]\u001b[A\n",
            " 31%|███       | 108/348 [00:00<00:00, 354.83it/s]\u001b[A\n",
            " 41%|████▏     | 144/348 [00:00<00:00, 355.12it/s]\u001b[A\n",
            " 52%|█████▏    | 180/348 [00:00<00:00, 355.11it/s]\u001b[A\n",
            " 62%|██████▏   | 216/348 [00:00<00:00, 356.54it/s]\u001b[A\n",
            " 72%|███████▏  | 252/348 [00:00<00:00, 355.35it/s]\u001b[A\n",
            " 83%|████████▎ | 288/348 [00:00<00:00, 353.51it/s]\u001b[A\n",
            "100%|██████████| 348/348 [00:00<00:00, 355.07it/s]\n",
            "8it [00:08,  1.10s/it]\n",
            "  0%|          | 0/368 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 38/368 [00:00<00:00, 372.05it/s]\u001b[A\n",
            " 21%|██        | 76/368 [00:00<00:00, 374.20it/s]\u001b[A\n",
            " 32%|███▏      | 116/368 [00:00<00:00, 380.81it/s]\u001b[A\n",
            " 42%|████▏     | 155/368 [00:00<00:00, 372.99it/s]\u001b[A\n",
            " 52%|█████▏    | 193/368 [00:00<00:00, 372.68it/s]\u001b[A\n",
            " 63%|██████▎   | 231/368 [00:00<00:00, 373.52it/s]\u001b[A\n",
            " 73%|███████▎  | 269/368 [00:00<00:00, 374.07it/s]\u001b[A\n",
            " 83%|████████▎ | 307/368 [00:00<00:00, 372.99it/s]\u001b[A\n",
            "100%|██████████| 368/368 [00:00<00:00, 373.40it/s]\n",
            "9it [00:09,  1.06s/it]\n",
            "  0%|          | 0/368 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 38/368 [00:00<00:00, 379.63it/s]\u001b[A\n",
            " 21%|██        | 76/368 [00:00<00:00, 371.57it/s]\u001b[A\n",
            " 31%|███       | 114/368 [00:00<00:00, 374.02it/s]\u001b[A\n",
            " 41%|████▏     | 152/368 [00:00<00:00, 374.67it/s]\u001b[A\n",
            " 52%|█████▏    | 191/368 [00:00<00:00, 377.00it/s]\u001b[A\n",
            " 62%|██████▏   | 229/368 [00:00<00:00, 374.34it/s]\u001b[A\n",
            " 73%|███████▎  | 267/368 [00:00<00:00, 374.94it/s]\u001b[A\n",
            " 83%|████████▎ | 305/368 [00:00<00:00, 370.36it/s]\u001b[A\n",
            "100%|██████████| 368/368 [00:00<00:00, 372.00it/s]\n",
            "10it [00:10,  1.04s/it]\n",
            "  0%|          | 0/364 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 39/364 [00:00<00:00, 386.41it/s]\u001b[A\n",
            " 21%|██▏       | 78/364 [00:00<00:00, 375.29it/s]\u001b[A\n",
            " 32%|███▏      | 116/364 [00:00<00:00, 375.86it/s]\u001b[A\n",
            " 42%|████▏     | 154/364 [00:00<00:00, 375.10it/s]\u001b[A\n",
            " 53%|█████▎    | 193/364 [00:00<00:00, 378.12it/s]\u001b[A\n",
            " 64%|██████▎   | 232/364 [00:00<00:00, 378.85it/s]\u001b[A\n",
            " 74%|███████▍  | 270/364 [00:00<00:00, 378.91it/s]\u001b[A\n",
            " 85%|████████▍ | 309/364 [00:00<00:00, 380.00it/s]\u001b[A\n",
            "100%|██████████| 364/364 [00:00<00:00, 378.18it/s]\n",
            "11it [00:11,  1.02s/it]\n",
            "  0%|          | 0/368 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 40/368 [00:00<00:00, 393.14it/s]\u001b[A\n",
            " 22%|██▏       | 80/368 [00:00<00:00, 389.18it/s]\u001b[A\n",
            " 32%|███▏      | 119/368 [00:00<00:00, 382.99it/s]\u001b[A\n",
            " 43%|████▎     | 159/368 [00:00<00:00, 387.57it/s]\u001b[A\n",
            " 54%|█████▍    | 198/368 [00:00<00:00, 388.19it/s]\u001b[A\n",
            " 64%|██████▍   | 237/368 [00:00<00:00, 384.01it/s]\u001b[A\n",
            " 75%|███████▌  | 276/368 [00:00<00:00, 385.49it/s]\u001b[A\n",
            " 86%|████████▌ | 315/368 [00:00<00:00, 385.01it/s]\u001b[A\n",
            "100%|██████████| 368/368 [00:00<00:00, 384.59it/s]\n",
            "12it [00:12,  1.00s/it]\n",
            "  0%|          | 0/368 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 39/368 [00:00<00:00, 383.51it/s]\u001b[A\n",
            " 21%|██        | 78/368 [00:00<00:00, 378.48it/s]\u001b[A\n",
            " 32%|███▏      | 116/368 [00:00<00:00, 378.21it/s]\u001b[A\n",
            " 42%|████▏     | 154/368 [00:00<00:00, 375.63it/s]\u001b[A\n",
            " 52%|█████▏    | 192/368 [00:00<00:00, 373.04it/s]\u001b[A\n",
            " 62%|██████▎   | 230/368 [00:00<00:00, 367.64it/s]\u001b[A\n",
            " 73%|███████▎  | 267/368 [00:00<00:00, 366.57it/s]\u001b[A\n",
            " 83%|████████▎ | 304/368 [00:00<00:00, 365.43it/s]\u001b[A\n",
            "100%|██████████| 368/368 [00:00<00:00, 371.64it/s]\n",
            "13it [00:13,  1.00it/s]\n",
            "  0%|          | 0/476 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 39/476 [00:00<00:01, 386.82it/s]\u001b[A\n",
            " 16%|█▋        | 78/476 [00:00<00:01, 386.95it/s]\u001b[A\n",
            " 25%|██▍       | 117/476 [00:00<00:00, 381.38it/s]\u001b[A\n",
            " 33%|███▎      | 156/476 [00:00<00:00, 383.54it/s]\u001b[A\n",
            " 41%|████      | 195/476 [00:00<00:00, 382.93it/s]\u001b[A\n",
            " 49%|████▉     | 234/476 [00:00<00:00, 380.32it/s]\u001b[A\n",
            " 57%|█████▋    | 273/476 [00:00<00:00, 381.15it/s]\u001b[A\n",
            " 66%|██████▌   | 312/476 [00:00<00:00, 380.41it/s]\u001b[A\n",
            " 74%|███████▎  | 351/476 [00:00<00:00, 383.04it/s]\u001b[A\n",
            " 82%|████████▏ | 390/476 [00:01<00:00, 384.19it/s]\u001b[A\n",
            " 90%|█████████ | 429/476 [00:01<00:00, 379.90it/s]\u001b[A\n",
            "100%|██████████| 476/476 [00:01<00:00, 381.06it/s]\n",
            "14it [00:14,  1.08s/it]\n",
            "  0%|          | 0/472 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 38/472 [00:00<00:01, 370.82it/s]\u001b[A\n",
            " 16%|█▌        | 76/472 [00:00<00:01, 367.37it/s]\u001b[A\n",
            " 24%|██▍       | 114/472 [00:00<00:00, 372.90it/s]\u001b[A\n",
            " 32%|███▏      | 153/472 [00:00<00:00, 379.42it/s]\u001b[A\n",
            " 40%|████      | 191/472 [00:00<00:00, 377.08it/s]\u001b[A\n",
            " 49%|████▊     | 229/472 [00:00<00:00, 377.46it/s]\u001b[A\n",
            " 57%|█████▋    | 267/472 [00:00<00:00, 376.05it/s]\u001b[A\n",
            " 65%|██████▍   | 306/472 [00:00<00:00, 378.11it/s]\u001b[A\n",
            " 73%|███████▎  | 345/472 [00:00<00:00, 380.62it/s]\u001b[A\n",
            " 81%|████████▏ | 384/472 [00:01<00:00, 380.72it/s]\u001b[A\n",
            " 90%|████████▉ | 423/472 [00:01<00:00, 377.57it/s]\u001b[A\n",
            "100%|██████████| 472/472 [00:01<00:00, 377.40it/s]\n",
            "15it [00:15,  1.13s/it]\n",
            "  0%|          | 0/360 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 38/360 [00:00<00:00, 377.75it/s]\u001b[A\n",
            " 21%|██        | 76/360 [00:00<00:00, 370.76it/s]\u001b[A\n",
            " 32%|███▏      | 114/360 [00:00<00:00, 368.46it/s]\u001b[A\n",
            " 42%|████▏     | 151/360 [00:00<00:00, 362.87it/s]\u001b[A\n",
            " 52%|█████▏    | 188/360 [00:00<00:00, 359.56it/s]\u001b[A\n",
            " 62%|██████▎   | 225/360 [00:00<00:00, 360.11it/s]\u001b[A\n",
            " 73%|███████▎  | 262/360 [00:00<00:00, 359.32it/s]\u001b[A\n",
            " 83%|████████▎ | 298/360 [00:00<00:00, 356.71it/s]\u001b[A\n",
            "100%|██████████| 360/360 [00:01<00:00, 357.29it/s]\n",
            "16it [00:16,  1.09s/it]\n",
            "  0%|          | 0/368 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 39/368 [00:00<00:00, 384.70it/s]\u001b[A\n",
            " 21%|██        | 78/368 [00:00<00:00, 364.50it/s]\u001b[A\n",
            " 31%|███▏      | 115/368 [00:00<00:00, 363.45it/s]\u001b[A\n",
            " 41%|████▏     | 152/368 [00:00<00:00, 364.42it/s]\u001b[A\n",
            " 52%|█████▏    | 190/368 [00:00<00:00, 368.86it/s]\u001b[A\n",
            " 62%|██████▏   | 228/368 [00:00<00:00, 372.33it/s]\u001b[A\n",
            " 72%|███████▏  | 266/368 [00:00<00:00, 370.34it/s]\u001b[A\n",
            " 83%|████████▎ | 304/368 [00:00<00:00, 367.94it/s]\u001b[A\n",
            "100%|██████████| 368/368 [00:00<00:00, 368.28it/s]\n",
            "17it [00:17,  1.07s/it]\n",
            "  0%|          | 0/480 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 37/480 [00:00<00:01, 364.52it/s]\u001b[A\n",
            " 15%|█▌        | 74/480 [00:00<00:01, 363.98it/s]\u001b[A\n",
            " 23%|██▎       | 112/480 [00:00<00:01, 367.35it/s]\u001b[A\n",
            " 31%|███       | 149/480 [00:00<00:00, 365.61it/s]\u001b[A\n",
            " 39%|███▉      | 186/480 [00:00<00:00, 364.50it/s]\u001b[A\n",
            " 46%|████▋     | 223/480 [00:00<00:00, 365.21it/s]\u001b[A\n",
            " 54%|█████▍    | 260/480 [00:00<00:00, 361.43it/s]\u001b[A\n",
            " 62%|██████▏   | 297/480 [00:00<00:00, 364.06it/s]\u001b[A\n",
            " 70%|██████▉   | 334/480 [00:00<00:00, 363.72it/s]\u001b[A\n",
            " 78%|███████▊  | 372/480 [00:01<00:00, 365.99it/s]\u001b[A\n",
            " 85%|████████▌ | 409/480 [00:01<00:00, 366.42it/s]\u001b[A\n",
            "100%|██████████| 480/480 [00:01<00:00, 364.68it/s]\n",
            "18it [00:19,  1.14s/it]\n",
            "  0%|          | 0/480 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 38/480 [00:00<00:01, 372.61it/s]\u001b[A\n",
            " 16%|█▋        | 78/480 [00:00<00:01, 386.94it/s]\u001b[A\n",
            " 25%|██▍       | 118/480 [00:00<00:00, 391.14it/s]\u001b[A\n",
            " 33%|███▎      | 158/480 [00:00<00:00, 385.62it/s]\u001b[A\n",
            " 41%|████      | 197/480 [00:00<00:00, 386.19it/s]\u001b[A\n",
            " 49%|████▉     | 237/480 [00:00<00:00, 388.26it/s]\u001b[A\n",
            " 57%|█████▊    | 276/480 [00:00<00:00, 386.38it/s]\u001b[A\n",
            " 66%|██████▌   | 315/480 [00:00<00:00, 384.06it/s]\u001b[A\n",
            " 74%|███████▍  | 354/480 [00:00<00:00, 384.28it/s]\u001b[A\n",
            " 82%|████████▏ | 393/480 [00:01<00:00, 383.90it/s]\u001b[A\n",
            " 90%|█████████ | 432/480 [00:01<00:00, 384.13it/s]\u001b[A\n",
            "100%|██████████| 480/480 [00:01<00:00, 383.69it/s]\n",
            "19it [00:20,  1.18s/it]\n",
            "  0%|          | 0/364 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 39/364 [00:00<00:00, 389.13it/s]\u001b[A\n",
            " 21%|██▏       | 78/364 [00:00<00:00, 386.08it/s]\u001b[A\n",
            " 32%|███▏      | 118/364 [00:00<00:00, 388.85it/s]\u001b[A\n",
            " 43%|████▎     | 157/364 [00:00<00:00, 385.09it/s]\u001b[A\n",
            " 54%|█████▍    | 196/364 [00:00<00:00, 384.42it/s]\u001b[A\n",
            " 65%|██████▍   | 236/364 [00:00<00:00, 388.45it/s]\u001b[A\n",
            " 76%|███████▌  | 275/364 [00:00<00:00, 388.40it/s]\u001b[A\n",
            " 86%|████████▋ | 314/364 [00:00<00:00, 386.70it/s]\u001b[A\n",
            "100%|██████████| 364/364 [00:00<00:00, 386.50it/s]\n",
            "20it [00:21,  1.11s/it]\n",
            "  0%|          | 0/368 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 38/368 [00:00<00:00, 376.71it/s]\u001b[A\n",
            " 21%|██        | 76/368 [00:00<00:00, 377.34it/s]\u001b[A\n",
            " 31%|███       | 114/368 [00:00<00:00, 371.99it/s]\u001b[A\n",
            " 41%|████▏     | 152/368 [00:00<00:00, 368.71it/s]\u001b[A\n",
            " 51%|█████▏    | 189/368 [00:00<00:00, 367.88it/s]\u001b[A\n",
            " 62%|██████▏   | 227/368 [00:00<00:00, 371.45it/s]\u001b[A\n",
            " 72%|███████▏  | 265/368 [00:00<00:00, 368.55it/s]\u001b[A\n",
            " 83%|████████▎ | 304/368 [00:00<00:00, 372.28it/s]\u001b[A\n",
            "100%|██████████| 368/368 [00:00<00:00, 368.09it/s]\n",
            "21it [00:22,  1.08s/it]\n",
            "  0%|          | 0/368 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 38/368 [00:00<00:00, 371.68it/s]\u001b[A\n",
            " 21%|██        | 76/368 [00:00<00:00, 371.72it/s]\u001b[A\n",
            " 31%|███       | 114/368 [00:00<00:00, 365.17it/s]\u001b[A\n",
            " 41%|████      | 151/368 [00:00<00:00, 366.34it/s]\u001b[A\n",
            " 51%|█████▏    | 189/368 [00:00<00:00, 368.33it/s]\u001b[A\n",
            " 61%|██████▏   | 226/368 [00:00<00:00, 366.88it/s]\u001b[A\n",
            " 71%|███████▏  | 263/368 [00:00<00:00, 366.75it/s]\u001b[A\n",
            " 82%|████████▏ | 300/368 [00:00<00:00, 365.22it/s]\u001b[A\n",
            "100%|██████████| 368/368 [00:01<00:00, 366.53it/s]\n",
            "22it [00:23,  1.06s/it]\n",
            "  0%|          | 0/472 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 39/472 [00:00<00:01, 385.40it/s]\u001b[A\n",
            " 17%|█▋        | 78/472 [00:00<00:01, 381.31it/s]\u001b[A\n",
            " 25%|██▌       | 118/472 [00:00<00:00, 386.04it/s]\u001b[A\n",
            " 33%|███▎      | 158/472 [00:00<00:00, 389.77it/s]\u001b[A\n",
            " 42%|████▏     | 197/472 [00:00<00:00, 387.07it/s]\u001b[A\n",
            " 50%|█████     | 236/472 [00:00<00:00, 385.04it/s]\u001b[A\n",
            " 58%|█████▊    | 275/472 [00:00<00:00, 377.47it/s]\u001b[A\n",
            " 67%|██████▋   | 314/472 [00:00<00:00, 378.34it/s]\u001b[A\n",
            " 75%|███████▍  | 353/472 [00:00<00:00, 380.17it/s]\u001b[A\n",
            " 83%|████████▎ | 392/472 [00:01<00:00, 377.20it/s]\u001b[A\n",
            " 91%|█████████▏| 431/472 [00:01<00:00, 379.67it/s]\u001b[A\n",
            "100%|██████████| 472/472 [00:01<00:00, 381.48it/s]\n",
            "23it [00:24,  1.11s/it]\n",
            "  0%|          | 0/476 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 39/476 [00:00<00:01, 380.42it/s]\u001b[A\n",
            " 16%|█▋        | 78/476 [00:00<00:01, 381.21it/s]\u001b[A\n",
            " 25%|██▍       | 117/476 [00:00<00:00, 376.19it/s]\u001b[A\n",
            " 33%|███▎      | 155/476 [00:00<00:00, 375.86it/s]\u001b[A\n",
            " 41%|████      | 194/476 [00:00<00:00, 379.33it/s]\u001b[A\n",
            " 49%|████▉     | 233/476 [00:00<00:00, 380.48it/s]\u001b[A\n",
            " 57%|█████▋    | 272/476 [00:00<00:00, 370.81it/s]\u001b[A\n",
            " 65%|██████▌   | 311/476 [00:00<00:00, 374.19it/s]\u001b[A\n",
            " 73%|███████▎  | 349/476 [00:00<00:00, 375.43it/s]\u001b[A\n",
            " 82%|████████▏ | 388/476 [00:01<00:00, 376.93it/s]\u001b[A\n",
            " 90%|████████▉ | 427/476 [00:01<00:00, 378.75it/s]\u001b[A\n",
            "100%|██████████| 476/476 [00:01<00:00, 376.00it/s]\n",
            "24it [00:25,  1.16s/it]\n",
            "  0%|          | 0/348 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 39/348 [00:00<00:00, 388.13it/s]\u001b[A\n",
            " 23%|██▎       | 79/348 [00:00<00:00, 390.98it/s]\u001b[A\n",
            " 34%|███▍      | 119/348 [00:00<00:00, 386.95it/s]\u001b[A\n",
            " 45%|████▌     | 158/348 [00:00<00:00, 387.44it/s]\u001b[A\n",
            " 57%|█████▋    | 198/348 [00:00<00:00, 388.08it/s]\u001b[A\n",
            " 68%|██████▊   | 237/348 [00:00<00:00, 383.26it/s]\u001b[A\n",
            " 79%|███████▉  | 276/348 [00:00<00:00, 382.47it/s]\u001b[A\n",
            "100%|██████████| 348/348 [00:00<00:00, 384.79it/s]\n",
            "25it [00:26,  1.08s/it]\n",
            "  0%|          | 0/292 [00:00<?, ?it/s]\u001b[A\n",
            " 13%|█▎        | 39/292 [00:00<00:00, 386.52it/s]\u001b[A\n",
            " 27%|██▋       | 78/292 [00:00<00:00, 378.81it/s]\u001b[A\n",
            " 40%|███▉      | 116/292 [00:00<00:00, 377.81it/s]\u001b[A\n",
            " 53%|█████▎    | 154/292 [00:00<00:00, 377.06it/s]\u001b[A\n",
            " 66%|██████▌   | 192/292 [00:00<00:00, 369.17it/s]\u001b[A\n",
            " 79%|███████▉  | 231/292 [00:00<00:00, 374.78it/s]\u001b[A\n",
            "100%|██████████| 292/292 [00:00<00:00, 376.42it/s]\n",
            "26it [00:27,  1.01it/s]\n",
            "  0%|          | 0/480 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 37/480 [00:00<00:01, 366.53it/s]\u001b[A\n",
            " 15%|█▌        | 74/480 [00:00<00:01, 366.71it/s]\u001b[A\n",
            " 23%|██▎       | 112/480 [00:00<00:00, 371.69it/s]\u001b[A\n",
            " 31%|███▏      | 150/480 [00:00<00:00, 373.35it/s]\u001b[A\n",
            " 39%|███▉      | 188/480 [00:00<00:00, 363.28it/s]\u001b[A\n",
            " 47%|████▋     | 225/480 [00:00<00:00, 352.65it/s]\u001b[A\n",
            " 55%|█████▍    | 262/480 [00:00<00:00, 357.38it/s]\u001b[A\n",
            " 62%|██████▏   | 299/480 [00:00<00:00, 360.47it/s]\u001b[A\n",
            " 70%|███████   | 337/480 [00:00<00:00, 365.12it/s]\u001b[A\n",
            " 78%|███████▊  | 375/480 [00:01<00:00, 367.13it/s]\u001b[A\n",
            " 86%|████████▌ | 412/480 [00:01<00:00, 365.36it/s]\u001b[A\n",
            "100%|██████████| 480/480 [00:01<00:00, 365.07it/s]\n",
            "27it [00:28,  1.09s/it]\n",
            "  0%|          | 0/368 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 40/368 [00:00<00:00, 396.16it/s]\u001b[A\n",
            " 22%|██▏       | 80/368 [00:00<00:00, 369.92it/s]\u001b[A\n",
            " 32%|███▏      | 119/368 [00:00<00:00, 377.69it/s]\u001b[A\n",
            " 43%|████▎     | 157/368 [00:00<00:00, 374.83it/s]\u001b[A\n",
            " 53%|█████▎    | 196/368 [00:00<00:00, 378.11it/s]\u001b[A\n",
            " 64%|██████▎   | 234/368 [00:00<00:00, 378.50it/s]\u001b[A\n",
            " 74%|███████▍  | 273/368 [00:00<00:00, 381.34it/s]\u001b[A\n",
            " 85%|████████▍ | 312/368 [00:00<00:00, 380.29it/s]\u001b[A\n",
            "100%|██████████| 368/368 [00:00<00:00, 378.22it/s]\n",
            "28it [00:29,  1.06s/it]\n",
            "  0%|          | 0/368 [00:00<?, ?it/s]\u001b[A\n",
            "  9%|▉         | 33/368 [00:00<00:01, 328.37it/s]\u001b[A\n",
            " 19%|█▉        | 70/368 [00:00<00:00, 350.59it/s]\u001b[A\n",
            " 29%|██▉       | 108/368 [00:00<00:00, 362.17it/s]\u001b[A\n",
            " 39%|███▉      | 145/368 [00:00<00:00, 351.91it/s]\u001b[A\n",
            " 50%|████▉     | 183/368 [00:00<00:00, 360.33it/s]\u001b[A\n",
            " 60%|██████    | 221/368 [00:00<00:00, 363.33it/s]\u001b[A\n",
            " 70%|███████   | 259/368 [00:00<00:00, 366.11it/s]\u001b[A\n",
            " 81%|████████  | 297/368 [00:00<00:00, 368.53it/s]\u001b[A\n",
            "100%|██████████| 368/368 [00:01<00:00, 363.79it/s]\n",
            "29it [00:30,  1.04s/it]\n",
            "  0%|          | 0/368 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 37/368 [00:00<00:00, 367.39it/s]\u001b[A\n",
            " 20%|██        | 74/368 [00:00<00:00, 364.67it/s]\u001b[A\n",
            " 30%|███       | 111/368 [00:00<00:00, 365.18it/s]\u001b[A\n",
            " 40%|████      | 148/368 [00:00<00:00, 363.80it/s]\u001b[A\n",
            " 50%|█████     | 185/368 [00:00<00:00, 365.29it/s]\u001b[A\n",
            " 60%|██████    | 222/368 [00:00<00:00, 363.74it/s]\u001b[A\n",
            " 70%|███████   | 259/368 [00:00<00:00, 365.34it/s]\u001b[A\n",
            " 80%|████████  | 296/368 [00:00<00:00, 363.03it/s]\u001b[A\n",
            "100%|██████████| 368/368 [00:01<00:00, 364.81it/s]\n",
            "30it [00:31,  1.03s/it]\n",
            "  0%|          | 0/472 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 39/472 [00:00<00:01, 388.38it/s]\u001b[A\n",
            " 17%|█▋        | 78/472 [00:00<00:01, 385.50it/s]\u001b[A\n",
            " 25%|██▍       | 117/472 [00:00<00:00, 364.75it/s]\u001b[A\n",
            " 33%|███▎      | 155/472 [00:00<00:00, 369.42it/s]\u001b[A\n",
            " 41%|████      | 193/472 [00:00<00:00, 372.02it/s]\u001b[A\n",
            " 49%|████▉     | 232/472 [00:00<00:00, 376.17it/s]\u001b[A\n",
            " 57%|█████▋    | 270/472 [00:00<00:00, 375.54it/s]\u001b[A\n",
            " 65%|██████▌   | 308/472 [00:00<00:00, 375.80it/s]\u001b[A\n",
            " 73%|███████▎  | 346/472 [00:00<00:00, 375.69it/s]\u001b[A\n",
            " 82%|████████▏ | 385/472 [00:01<00:00, 378.26it/s]\u001b[A\n",
            " 90%|████████▉ | 423/472 [00:01<00:00, 377.51it/s]\u001b[A\n",
            "100%|██████████| 472/472 [00:01<00:00, 375.37it/s]\n",
            "31it [00:33,  1.10s/it]\n",
            "  0%|          | 0/472 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 37/472 [00:00<00:01, 369.82it/s]\u001b[A\n",
            " 16%|█▌        | 74/472 [00:00<00:01, 358.29it/s]\u001b[A\n",
            " 24%|██▎       | 112/472 [00:00<00:00, 364.29it/s]\u001b[A\n",
            " 32%|███▏      | 150/472 [00:00<00:00, 368.32it/s]\u001b[A\n",
            " 40%|███▉      | 188/472 [00:00<00:00, 370.70it/s]\u001b[A\n",
            " 48%|████▊     | 226/472 [00:00<00:00, 372.55it/s]\u001b[A\n",
            " 56%|█████▌    | 265/472 [00:00<00:00, 376.04it/s]\u001b[A\n",
            " 64%|██████▍   | 303/472 [00:00<00:00, 371.38it/s]\u001b[A\n",
            " 72%|███████▏  | 341/472 [00:00<00:00, 370.96it/s]\u001b[A\n",
            " 80%|████████  | 379/472 [00:01<00:00, 371.43it/s]\u001b[A\n",
            " 88%|████████▊ | 417/472 [00:01<00:00, 372.57it/s]\u001b[A\n",
            "100%|██████████| 472/472 [00:01<00:00, 371.05it/s]\n",
            "32it [00:34,  1.15s/it]\n",
            "  0%|          | 0/364 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 38/364 [00:00<00:00, 370.88it/s]\u001b[A\n",
            " 21%|██        | 76/364 [00:00<00:00, 371.81it/s]\u001b[A\n",
            " 31%|███▏      | 114/364 [00:00<00:00, 371.89it/s]\u001b[A\n",
            " 42%|████▏     | 152/364 [00:00<00:00, 370.32it/s]\u001b[A\n",
            " 52%|█████▏    | 190/364 [00:00<00:00, 370.41it/s]\u001b[A\n",
            " 63%|██████▎   | 228/364 [00:00<00:00, 371.53it/s]\u001b[A\n",
            " 73%|███████▎  | 266/364 [00:00<00:00, 370.82it/s]\u001b[A\n",
            " 84%|████████▎ | 304/364 [00:00<00:00, 355.52it/s]\u001b[A\n",
            "100%|██████████| 364/364 [00:01<00:00, 363.27it/s]\n",
            "33it [00:35,  1.11s/it]\n",
            "  0%|          | 0/464 [00:00<?, ?it/s]\u001b[A\n",
            "  9%|▊         | 40/464 [00:00<00:01, 393.35it/s]\u001b[A\n",
            " 17%|█▋        | 80/464 [00:00<00:01, 383.20it/s]\u001b[A\n",
            " 26%|██▌       | 119/464 [00:00<00:00, 377.71it/s]\u001b[A\n",
            " 34%|███▍      | 158/464 [00:00<00:00, 379.02it/s]\u001b[A\n",
            " 42%|████▏     | 196/464 [00:00<00:00, 375.92it/s]\u001b[A\n",
            " 50%|█████     | 234/464 [00:00<00:00, 373.75it/s]\u001b[A\n",
            " 59%|█████▊    | 272/464 [00:00<00:00, 373.65it/s]\u001b[A\n",
            " 67%|██████▋   | 310/464 [00:00<00:00, 374.56it/s]\u001b[A\n",
            " 75%|███████▌  | 349/464 [00:00<00:00, 377.16it/s]\u001b[A\n",
            " 83%|████████▎ | 387/464 [00:01<00:00, 377.24it/s]\u001b[A\n",
            " 92%|█████████▏| 425/464 [00:01<00:00, 378.07it/s]\u001b[A\n",
            "100%|██████████| 464/464 [00:01<00:00, 376.94it/s]\n",
            "34it [00:36,  1.15s/it]\n",
            "  0%|          | 0/368 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 41/368 [00:00<00:00, 399.07it/s]\u001b[A\n",
            " 22%|██▏       | 81/368 [00:00<00:00, 394.70it/s]\u001b[A\n",
            " 33%|███▎      | 121/368 [00:00<00:00, 387.47it/s]\u001b[A\n",
            " 44%|████▍     | 161/368 [00:00<00:00, 389.46it/s]\u001b[A\n",
            " 54%|█████▍    | 200/368 [00:00<00:00, 374.03it/s]\u001b[A\n",
            " 65%|██████▍   | 239/368 [00:00<00:00, 377.26it/s]\u001b[A\n",
            " 76%|███████▌  | 278/368 [00:00<00:00, 379.11it/s]\u001b[A\n",
            " 86%|████████▌ | 317/368 [00:00<00:00, 381.84it/s]\u001b[A\n",
            "100%|██████████| 368/368 [00:00<00:00, 382.79it/s]\n",
            "35it [00:37,  1.09s/it]\n",
            "  0%|          | 0/480 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 38/480 [00:00<00:01, 374.20it/s]\u001b[A\n",
            " 16%|█▌        | 76/480 [00:00<00:01, 372.18it/s]\u001b[A\n",
            " 24%|██▍       | 114/480 [00:00<00:00, 373.08it/s]\u001b[A\n",
            " 32%|███▏      | 153/480 [00:00<00:00, 377.24it/s]\u001b[A\n",
            " 40%|███▉      | 191/480 [00:00<00:00, 377.39it/s]\u001b[A\n",
            " 48%|████▊     | 229/480 [00:00<00:00, 374.19it/s]\u001b[A\n",
            " 56%|█████▌    | 267/480 [00:00<00:00, 371.68it/s]\u001b[A\n",
            " 64%|██████▎   | 305/480 [00:00<00:00, 369.96it/s]\u001b[A\n",
            " 71%|███████▏  | 343/480 [00:00<00:00, 365.83it/s]\u001b[A\n",
            " 79%|███████▉  | 381/480 [00:01<00:00, 369.66it/s]\u001b[A\n",
            " 87%|████████▋ | 418/480 [00:01<00:00, 369.68it/s]\u001b[A\n",
            "100%|██████████| 480/480 [00:01<00:00, 371.09it/s]\n",
            "36it [00:38,  1.15s/it]\n",
            "  0%|          | 0/480 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 38/480 [00:00<00:01, 371.54it/s]\u001b[A\n",
            " 16%|█▌        | 76/480 [00:00<00:01, 357.90it/s]\u001b[A\n",
            " 23%|██▎       | 112/480 [00:00<00:01, 358.77it/s]\u001b[A\n",
            " 31%|███▏      | 150/480 [00:00<00:00, 364.68it/s]\u001b[A\n",
            " 39%|███▉      | 187/480 [00:00<00:00, 363.70it/s]\u001b[A\n",
            " 47%|████▋     | 224/480 [00:00<00:00, 361.85it/s]\u001b[A\n",
            " 54%|█████▍    | 261/480 [00:00<00:00, 363.41it/s]\u001b[A\n",
            " 62%|██████▏   | 298/480 [00:00<00:00, 361.71it/s]\u001b[A\n",
            " 70%|██████▉   | 335/480 [00:00<00:00, 359.82it/s]\u001b[A\n",
            " 77%|███████▋  | 371/480 [00:01<00:00, 359.64it/s]\u001b[A\n",
            " 85%|████████▍ | 407/480 [00:01<00:00, 357.81it/s]\u001b[A\n",
            " 92%|█████████▏| 443/480 [00:01<00:00, 354.82it/s]\u001b[A\n",
            "100%|██████████| 480/480 [00:01<00:00, 358.51it/s]\n",
            "37it [00:40,  1.21s/it]\n",
            "  0%|          | 0/476 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 36/476 [00:00<00:01, 354.06it/s]\u001b[A\n",
            " 16%|█▌        | 74/476 [00:00<00:01, 364.94it/s]\u001b[A\n",
            " 24%|██▎       | 113/476 [00:00<00:00, 376.26it/s]\u001b[A\n",
            " 32%|███▏      | 152/476 [00:00<00:00, 378.10it/s]\u001b[A\n",
            " 40%|████      | 191/476 [00:00<00:00, 380.05it/s]\u001b[A\n",
            " 48%|████▊     | 230/476 [00:00<00:00, 379.64it/s]\u001b[A\n",
            " 56%|█████▋    | 268/476 [00:00<00:00, 377.31it/s]\u001b[A\n",
            " 64%|██████▍   | 306/476 [00:00<00:00, 374.78it/s]\u001b[A\n",
            " 72%|███████▏  | 345/476 [00:00<00:00, 378.02it/s]\u001b[A\n",
            " 81%|████████  | 384/476 [00:01<00:00, 379.84it/s]\u001b[A\n",
            " 89%|████████▊ | 422/476 [00:01<00:00, 375.50it/s]\u001b[A\n",
            "100%|██████████| 476/476 [00:01<00:00, 376.11it/s]\n",
            "38it [00:41,  1.23s/it]\n",
            "  0%|          | 0/368 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 39/368 [00:00<00:00, 384.38it/s]\u001b[A\n",
            " 21%|██▏       | 79/368 [00:00<00:00, 388.42it/s]\u001b[A\n",
            " 32%|███▏      | 119/368 [00:00<00:00, 390.82it/s]\u001b[A\n",
            " 43%|████▎     | 159/368 [00:00<00:00, 388.35it/s]\u001b[A\n",
            " 54%|█████▍    | 198/368 [00:00<00:00, 383.87it/s]\u001b[A\n",
            " 64%|██████▍   | 237/368 [00:00<00:00, 382.11it/s]\u001b[A\n",
            " 75%|███████▌  | 276/368 [00:00<00:00, 372.15it/s]\u001b[A\n",
            " 86%|████████▌ | 315/368 [00:00<00:00, 377.22it/s]\u001b[A\n",
            "100%|██████████| 368/368 [00:00<00:00, 379.38it/s]\n",
            "39it [00:42,  1.15s/it]\n",
            "  0%|          | 0/364 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 38/364 [00:00<00:00, 374.36it/s]\u001b[A\n",
            " 21%|██        | 76/364 [00:00<00:00, 368.07it/s]\u001b[A\n",
            " 31%|███       | 113/364 [00:00<00:00, 366.63it/s]\u001b[A\n",
            " 41%|████▏     | 151/364 [00:00<00:00, 367.57it/s]\u001b[A\n",
            " 52%|█████▏    | 189/364 [00:00<00:00, 370.65it/s]\u001b[A\n",
            " 62%|██████▏   | 227/364 [00:00<00:00, 368.28it/s]\u001b[A\n",
            " 73%|███████▎  | 264/364 [00:00<00:00, 368.33it/s]\u001b[A\n",
            " 83%|████████▎ | 302/364 [00:00<00:00, 371.81it/s]\u001b[A\n",
            "100%|██████████| 364/364 [00:00<00:00, 369.96it/s]\n",
            "40it [00:43,  1.10s/it]\n",
            "  0%|          | 0/480 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 39/480 [00:00<00:01, 379.32it/s]\u001b[A\n",
            " 16%|█▌        | 77/480 [00:00<00:01, 370.87it/s]\u001b[A\n",
            " 24%|██▍       | 115/480 [00:00<00:00, 370.47it/s]\u001b[A\n",
            " 32%|███▏      | 153/480 [00:00<00:00, 373.22it/s]\u001b[A\n",
            " 40%|███▉      | 191/480 [00:00<00:00, 372.32it/s]\u001b[A\n",
            " 48%|████▊     | 229/480 [00:00<00:00, 364.36it/s]\u001b[A\n",
            " 55%|█████▌    | 266/480 [00:00<00:00, 366.10it/s]\u001b[A\n",
            " 63%|██████▎   | 303/480 [00:00<00:00, 364.86it/s]\u001b[A\n",
            " 71%|███████   | 341/480 [00:00<00:00, 366.95it/s]\u001b[A\n",
            " 79%|███████▉  | 378/480 [00:01<00:00, 365.70it/s]\u001b[A\n",
            " 86%|████████▋ | 415/480 [00:01<00:00, 366.13it/s]\u001b[A\n",
            "100%|██████████| 480/480 [00:01<00:00, 367.90it/s]\n",
            "41it [00:44,  1.16s/it]\n",
            "  0%|          | 0/480 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 38/480 [00:00<00:01, 376.14it/s]\u001b[A\n",
            " 16%|█▌        | 76/480 [00:00<00:01, 376.88it/s]\u001b[A\n",
            " 24%|██▍       | 115/480 [00:00<00:00, 381.44it/s]\u001b[A\n",
            " 32%|███▏      | 155/480 [00:00<00:00, 385.41it/s]\u001b[A\n",
            " 40%|████      | 194/480 [00:00<00:00, 380.18it/s]\u001b[A\n",
            " 49%|████▊     | 233/480 [00:00<00:00, 377.84it/s]\u001b[A\n",
            " 56%|█████▋    | 271/480 [00:00<00:00, 377.73it/s]\u001b[A\n",
            " 64%|██████▍   | 309/480 [00:00<00:00, 378.18it/s]\u001b[A\n",
            " 72%|███████▎  | 348/480 [00:00<00:00, 379.33it/s]\u001b[A\n",
            " 80%|████████  | 386/480 [00:01<00:00, 377.36it/s]\u001b[A\n",
            " 88%|████████▊ | 424/480 [00:01<00:00, 375.92it/s]\u001b[A\n",
            "100%|██████████| 480/480 [00:01<00:00, 378.66it/s]\n",
            "42it [00:45,  1.20s/it]\n",
            "  0%|          | 0/480 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 40/480 [00:00<00:01, 390.44it/s]\u001b[A\n",
            " 17%|█▋        | 80/480 [00:00<00:01, 391.15it/s]\u001b[A\n",
            " 25%|██▌       | 120/480 [00:00<00:00, 389.96it/s]\u001b[A\n",
            " 33%|███▎      | 159/480 [00:00<00:00, 382.87it/s]\u001b[A\n",
            " 41%|████▏     | 199/480 [00:00<00:00, 385.51it/s]\u001b[A\n",
            " 50%|████▉     | 239/480 [00:00<00:00, 387.80it/s]\u001b[A\n",
            " 58%|█████▊    | 278/480 [00:00<00:00, 385.11it/s]\u001b[A\n",
            " 66%|██████▌   | 317/480 [00:00<00:00, 384.26it/s]\u001b[A\n",
            " 74%|███████▍  | 356/480 [00:00<00:00, 384.14it/s]\u001b[A\n",
            " 82%|████████▏ | 395/480 [00:01<00:00, 384.01it/s]\u001b[A\n",
            " 90%|█████████ | 434/480 [00:01<00:00, 383.94it/s]\u001b[A\n",
            "100%|██████████| 480/480 [00:01<00:00, 383.97it/s]\n",
            "43it [00:47,  1.21s/it]\n",
            "  0%|          | 0/364 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 39/364 [00:00<00:00, 385.71it/s]\u001b[A\n",
            " 21%|██▏       | 78/364 [00:00<00:00, 377.38it/s]\u001b[A\n",
            " 32%|███▏      | 117/364 [00:00<00:00, 379.75it/s]\u001b[A\n",
            " 43%|████▎     | 155/364 [00:00<00:00, 378.60it/s]\u001b[A\n",
            " 54%|█████▎    | 195/364 [00:00<00:00, 384.01it/s]\u001b[A\n",
            " 64%|██████▍   | 234/364 [00:00<00:00, 383.40it/s]\u001b[A\n",
            " 75%|███████▌  | 273/364 [00:00<00:00, 384.26it/s]\u001b[A\n",
            " 86%|████████▌ | 312/364 [00:00<00:00, 384.62it/s]\u001b[A\n",
            "100%|██████████| 364/364 [00:00<00:00, 381.33it/s]\n",
            "44it [00:48,  1.14s/it]\n",
            "  0%|          | 0/344 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 36/344 [00:00<00:00, 349.05it/s]\u001b[A\n",
            " 21%|██        | 72/344 [00:00<00:00, 352.44it/s]\u001b[A\n",
            " 32%|███▏      | 109/344 [00:00<00:00, 359.79it/s]\u001b[A\n",
            " 42%|████▏     | 146/344 [00:00<00:00, 360.97it/s]\u001b[A\n",
            " 53%|█████▎    | 183/344 [00:00<00:00, 363.80it/s]\u001b[A\n",
            " 64%|██████▍   | 220/344 [00:00<00:00, 359.91it/s]\u001b[A\n",
            " 75%|███████▍  | 257/344 [00:00<00:00, 361.33it/s]\u001b[A\n",
            " 85%|████████▌ | 294/344 [00:00<00:00, 362.81it/s]\u001b[A\n",
            "100%|██████████| 344/344 [00:00<00:00, 361.55it/s]\n",
            "45it [00:49,  1.08s/it]\n",
            "  0%|          | 0/472 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 37/472 [00:00<00:01, 362.69it/s]\u001b[A\n",
            " 16%|█▌        | 75/472 [00:00<00:01, 369.94it/s]\u001b[A\n",
            " 24%|██▎       | 112/472 [00:00<00:00, 367.74it/s]\u001b[A\n",
            " 32%|███▏      | 150/472 [00:00<00:00, 369.50it/s]\u001b[A\n",
            " 40%|███▉      | 187/472 [00:00<00:00, 367.76it/s]\u001b[A\n",
            " 47%|████▋     | 224/472 [00:00<00:00, 367.33it/s]\u001b[A\n",
            " 55%|█████▌    | 261/472 [00:00<00:00, 367.11it/s]\u001b[A\n",
            " 63%|██████▎   | 298/472 [00:00<00:00, 366.12it/s]\u001b[A\n",
            " 71%|███████   | 336/472 [00:00<00:00, 367.66it/s]\u001b[A\n",
            " 79%|███████▉  | 373/472 [00:01<00:00, 366.73it/s]\u001b[A\n",
            " 87%|████████▋ | 411/472 [00:01<00:00, 369.62it/s]\u001b[A\n",
            "100%|██████████| 472/472 [00:01<00:00, 367.52it/s]\n",
            "46it [00:50,  1.14s/it]\n",
            "  0%|          | 0/320 [00:00<?, ?it/s]\u001b[A\n",
            " 12%|█▏        | 38/320 [00:00<00:00, 378.08it/s]\u001b[A\n",
            " 24%|██▍       | 76/320 [00:00<00:00, 372.12it/s]\u001b[A\n",
            " 36%|███▌      | 114/320 [00:00<00:00, 371.96it/s]\u001b[A\n",
            " 48%|████▊     | 152/320 [00:00<00:00, 369.99it/s]\u001b[A\n",
            " 59%|█████▉    | 190/320 [00:00<00:00, 368.52it/s]\u001b[A\n",
            " 71%|███████▏  | 228/320 [00:00<00:00, 370.54it/s]\u001b[A\n",
            " 83%|████████▎ | 266/320 [00:00<00:00, 370.35it/s]\u001b[A\n",
            "100%|██████████| 320/320 [00:00<00:00, 370.80it/s]\n",
            "47it [00:51,  1.06s/it]\n",
            "  0%|          | 0/340 [00:00<?, ?it/s]\u001b[A\n",
            " 11%|█         | 37/340 [00:00<00:00, 365.69it/s]\u001b[A\n",
            " 22%|██▏       | 75/340 [00:00<00:00, 372.58it/s]\u001b[A\n",
            " 33%|███▎      | 113/340 [00:00<00:00, 370.08it/s]\u001b[A\n",
            " 44%|████▍     | 151/340 [00:00<00:00, 369.30it/s]\u001b[A\n",
            " 55%|█████▌    | 188/340 [00:00<00:00, 364.48it/s]\u001b[A\n",
            " 66%|██████▌   | 225/340 [00:00<00:00, 362.51it/s]\u001b[A\n",
            " 77%|███████▋  | 263/340 [00:00<00:00, 364.64it/s]\u001b[A\n",
            " 88%|████████▊ | 300/340 [00:00<00:00, 364.04it/s]\u001b[A\n",
            "100%|██████████| 340/340 [00:00<00:00, 366.22it/s]\n",
            "48it [00:52,  1.02s/it]\n",
            "  0%|          | 0/480 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 40/480 [00:00<00:01, 392.56it/s]\u001b[A\n",
            " 17%|█▋        | 80/480 [00:00<00:01, 376.15it/s]\u001b[A\n",
            " 25%|██▍       | 118/480 [00:00<00:00, 369.49it/s]\u001b[A\n",
            " 32%|███▏      | 155/480 [00:00<00:00, 365.05it/s]\u001b[A\n",
            " 40%|████      | 192/480 [00:00<00:00, 360.51it/s]\u001b[A\n",
            " 48%|████▊     | 230/480 [00:00<00:00, 365.20it/s]\u001b[A\n",
            " 56%|█████▌    | 268/480 [00:00<00:00, 367.54it/s]\u001b[A\n",
            " 64%|██████▍   | 306/480 [00:00<00:00, 369.89it/s]\u001b[A\n",
            " 72%|███████▏  | 344/480 [00:00<00:00, 372.74it/s]\u001b[A\n",
            " 80%|███████▉  | 382/480 [00:01<00:00, 372.67it/s]\u001b[A\n",
            " 88%|████████▊ | 420/480 [00:01<00:00, 373.52it/s]\u001b[A\n",
            "100%|██████████| 480/480 [00:01<00:00, 370.58it/s]\n",
            "49it [00:53,  1.11s/it]\n",
            "  0%|          | 0/364 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 38/364 [00:00<00:00, 378.94it/s]\u001b[A\n",
            " 21%|██        | 76/364 [00:00<00:00, 368.85it/s]\u001b[A\n",
            " 31%|███▏      | 114/364 [00:00<00:00, 372.95it/s]\u001b[A\n",
            " 42%|████▏     | 152/364 [00:00<00:00, 370.69it/s]\u001b[A\n",
            " 52%|█████▏    | 190/364 [00:00<00:00, 365.45it/s]\u001b[A\n",
            " 62%|██████▏   | 227/364 [00:00<00:00, 366.27it/s]\u001b[A\n",
            " 73%|███████▎  | 265/364 [00:00<00:00, 369.74it/s]\u001b[A\n",
            " 83%|████████▎ | 302/364 [00:00<00:00, 358.94it/s]\u001b[A\n",
            "100%|██████████| 364/364 [00:00<00:00, 367.23it/s]\n",
            "50it [00:54,  1.07s/it]\n",
            "  0%|          | 0/480 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 39/480 [00:00<00:01, 385.63it/s]\u001b[A\n",
            " 16%|█▋        | 78/480 [00:00<00:01, 374.92it/s]\u001b[A\n",
            " 24%|██▍       | 116/480 [00:00<00:00, 372.65it/s]\u001b[A\n",
            " 32%|███▏      | 154/480 [00:00<00:00, 374.36it/s]\u001b[A\n",
            " 40%|████      | 192/480 [00:00<00:00, 376.29it/s]\u001b[A\n",
            " 48%|████▊     | 230/480 [00:00<00:00, 368.09it/s]\u001b[A\n",
            " 56%|█████▌    | 267/480 [00:00<00:00, 368.64it/s]\u001b[A\n",
            " 63%|██████▎   | 304/480 [00:00<00:00, 365.46it/s]\u001b[A\n",
            " 71%|███████▏  | 343/480 [00:00<00:00, 370.81it/s]\u001b[A\n",
            " 79%|███████▉  | 381/480 [00:01<00:00, 368.60it/s]\u001b[A\n",
            " 87%|████████▋ | 418/480 [00:01<00:00, 368.52it/s]\u001b[A\n",
            "100%|██████████| 480/480 [00:01<00:00, 371.14it/s]\n",
            "51it [00:55,  1.09s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== HOG Extraction Completed! ======\n",
            "(20736, 1764)\n",
            "====== HOG Extraction Starts! ======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "100%|██████████| 30/30 [00:00<00:00, 378.22it/s]\n",
            "\n",
            "100%|██████████| 30/30 [00:00<00:00, 389.12it/s]\n",
            "3it [00:00, 17.91it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 404.65it/s]\n",
            "\n",
            "100%|██████████| 23/23 [00:00<00:00, 401.69it/s]\n",
            "5it [00:00, 16.09it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 396.59it/s]\n",
            "\n",
            "100%|██████████| 30/30 [00:00<00:00, 351.20it/s]\n",
            "7it [00:00, 14.08it/s]\n",
            "100%|██████████| 21/21 [00:00<00:00, 348.02it/s]\n",
            "\n",
            "100%|██████████| 23/23 [00:00<00:00, 387.15it/s]\n",
            "9it [00:00, 14.68it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 387.10it/s]\n",
            "\n",
            "100%|██████████| 23/23 [00:00<00:00, 359.91it/s]\n",
            "11it [00:00, 14.92it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 384.45it/s]\n",
            "\n",
            "100%|██████████| 23/23 [00:00<00:00, 372.12it/s]\n",
            "13it [00:00, 15.12it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 392.62it/s]\n",
            "\n",
            "100%|██████████| 29/29 [00:00<00:00, 378.18it/s]\n",
            "15it [00:01, 14.18it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 334.50it/s]\n",
            "\n",
            "100%|██████████| 23/23 [00:00<00:00, 384.98it/s]\n",
            "17it [00:01, 14.36it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 379.29it/s]\n",
            "\n",
            "100%|██████████| 30/30 [00:00<00:00, 398.25it/s]\n",
            "19it [00:01, 13.72it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 403.45it/s]\n",
            "\n",
            "100%|██████████| 23/23 [00:00<00:00, 373.54it/s]\n",
            "21it [00:01, 14.36it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 386.54it/s]\n",
            "\n",
            "100%|██████████| 30/30 [00:00<00:00, 380.05it/s]\n",
            "23it [00:01, 14.20it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 396.05it/s]\n",
            "\n",
            "100%|██████████| 21/21 [00:00<00:00, 382.06it/s]\n",
            "25it [00:01, 14.33it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 367.81it/s]\n",
            "\n",
            "100%|██████████| 30/30 [00:00<00:00, 378.61it/s]\n",
            "27it [00:01, 14.48it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 389.93it/s]\n",
            "\n",
            "100%|██████████| 23/23 [00:00<00:00, 385.31it/s]\n",
            "29it [00:01, 14.90it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 378.95it/s]\n",
            "\n",
            "100%|██████████| 29/29 [00:00<00:00, 347.13it/s]\n",
            "31it [00:02, 14.40it/s]\n",
            "100%|██████████| 29/29 [00:00<00:00, 390.13it/s]\n",
            "\n",
            "100%|██████████| 22/22 [00:00<00:00, 387.49it/s]\n",
            "33it [00:02, 14.43it/s]\n",
            "100%|██████████| 29/29 [00:00<00:00, 382.11it/s]\n",
            "\n",
            "100%|██████████| 23/23 [00:00<00:00, 395.78it/s]\n",
            "35it [00:02, 14.36it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 386.01it/s]\n",
            "\n",
            "100%|██████████| 30/30 [00:00<00:00, 379.63it/s]\n",
            "37it [00:02, 13.66it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 386.66it/s]\n",
            "\n",
            "100%|██████████| 23/23 [00:00<00:00, 366.99it/s]\n",
            "39it [00:02, 13.67it/s]\n",
            "100%|██████████| 22/22 [00:00<00:00, 366.85it/s]\n",
            "\n",
            "100%|██████████| 30/30 [00:00<00:00, 344.04it/s]\n",
            "41it [00:02, 13.50it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 385.28it/s]\n",
            "\n",
            "100%|██████████| 30/30 [00:00<00:00, 394.12it/s]\n",
            "43it [00:03, 13.19it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 392.01it/s]\n",
            "\n",
            "100%|██████████| 21/21 [00:00<00:00, 379.14it/s]\n",
            "45it [00:03, 14.06it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 367.29it/s]\n",
            "\n",
            "100%|██████████| 20/20 [00:00<00:00, 373.72it/s]\n",
            "47it [00:03, 14.10it/s]\n",
            "100%|██████████| 21/21 [00:00<00:00, 393.51it/s]\n",
            "\n",
            "100%|██████████| 30/30 [00:00<00:00, 385.37it/s]\n",
            "49it [00:03, 14.24it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 377.14it/s]\n",
            "\n",
            "100%|██████████| 30/30 [00:00<00:00, 374.26it/s]\n",
            "51it [00:03, 14.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== HOG Extraction Completed! ======\n",
            "(1296, 1764)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# train set:\n",
        "input_dir = '/content/CS610_AML_Group_Project/augmented_train_images'\n",
        "print('====== HOG Extraction Starts! ======')\n",
        "hogged_train, filenames_train = extract_hog_features_recursive(input_dir)\n",
        "print('====== HOG Extraction Completed! ======')\n",
        "print(hogged_train.shape)  # (num_images, hog_feature_dim)\n",
        "\n",
        "# test set:\n",
        "input_dir = '/content/CS610_AML_Group_Project/split_images/test'\n",
        "print('====== HOG Extraction Starts! ======')\n",
        "hogged_test, filenames_test = extract_hog_features_recursive(input_dir)\n",
        "print('====== HOG Extraction Completed! ======')\n",
        "print(hogged_test.shape)  # (num_images, hog_feature_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BFvEr97JWNS7"
      },
      "outputs": [],
      "source": [
        "#Labeling\n",
        "y_train = [f.split(os.sep)[0] for f in filenames_train]\n",
        "\n",
        "#split data into train_test split\n",
        "x_train = hogged_train.astype(np.float32)\n",
        "y_train = np.array(y_train)\n",
        "y_train, uniques = pd.factorize(y_train)\n",
        "x_train = pd.DataFrame(x_train, dtype=np.float32)\n",
        "y_train = pd.Series(y_train, dtype=np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Lu1tBZQxWNS7"
      },
      "outputs": [],
      "source": [
        "#Labeling\n",
        "y_test = [f.split(os.sep)[0] for f in filenames_test]\n",
        "\n",
        "#split data into train_test split\n",
        "x_test = hogged_test.astype(np.float32)\n",
        "y_test = np.array(y_test)\n",
        "y_test, uniques = pd.factorize(y_test)\n",
        "x_test = pd.DataFrame(x_test, dtype=np.float32)\n",
        "y_test = pd.Series(y_test, dtype=np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C5W_CtrWNS8",
        "outputId": "141bd1da-5e21-447a-a7e1-38d6f705fc14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Train Samples: 20736\n",
            "Number of Train Labels: 50\n",
            "Train Label Distribution:\n",
            "16    480\n",
            "17    480\n",
            "34    480\n",
            "35    480\n",
            "40    480\n",
            "39    480\n",
            "41    480\n",
            "49    480\n",
            "47    480\n",
            "25    480\n",
            "12    476\n",
            "1     476\n",
            "22    476\n",
            "36    476\n",
            "13    472\n",
            "4     472\n",
            "0     472\n",
            "2     472\n",
            "30    472\n",
            "5     472\n",
            "21    472\n",
            "29    472\n",
            "44    472\n",
            "32    464\n",
            "19    368\n",
            "10    368\n",
            "3     368\n",
            "37    368\n",
            "27    368\n",
            "15    368\n",
            "7     368\n",
            "8     368\n",
            "11    368\n",
            "20    368\n",
            "26    368\n",
            "33    368\n",
            "28    368\n",
            "9     364\n",
            "48    364\n",
            "42    364\n",
            "31    364\n",
            "18    364\n",
            "38    364\n",
            "14    360\n",
            "6     348\n",
            "23    348\n",
            "43    344\n",
            "46    340\n",
            "45    320\n",
            "24    292\n",
            "Name: count, dtype: int64\n",
            "Number of Test Samples: 1296\n",
            "Number of Test Labels: 50\n",
            "Test Label Distribution:\n",
            "0     30\n",
            "1     30\n",
            "2     30\n",
            "4     30\n",
            "5     30\n",
            "12    30\n",
            "17    30\n",
            "16    30\n",
            "47    30\n",
            "44    30\n",
            "49    30\n",
            "35    30\n",
            "36    30\n",
            "39    30\n",
            "41    30\n",
            "25    30\n",
            "22    30\n",
            "21    30\n",
            "40    30\n",
            "34    30\n",
            "30    29\n",
            "13    29\n",
            "29    29\n",
            "32    29\n",
            "18    23\n",
            "10    23\n",
            "3     23\n",
            "14    23\n",
            "11    23\n",
            "15    23\n",
            "7     23\n",
            "8     23\n",
            "9     23\n",
            "20    23\n",
            "42    23\n",
            "37    23\n",
            "48    23\n",
            "33    23\n",
            "26    23\n",
            "19    23\n",
            "27    23\n",
            "28    23\n",
            "31    22\n",
            "38    22\n",
            "6     21\n",
            "23    21\n",
            "46    21\n",
            "43    21\n",
            "45    20\n",
            "24    18\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#Check if data is prepared successfully\n",
        "print(\"Number of Train Samples:\", len(y_train))\n",
        "print(\"Number of Train Labels:\", len(np.unique(y_train)))\n",
        "counts = y_train.value_counts()\n",
        "print(\"Train Label Distribution:\")\n",
        "print(counts)\n",
        "\n",
        "print(\"Number of Test Samples:\", len(y_test))\n",
        "print(\"Number of Test Labels:\", len(np.unique(y_test)))\n",
        "counts = y_test.value_counts()\n",
        "print(\"Test Label Distribution:\")\n",
        "print(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o29zHwGWNS8"
      },
      "source": [
        "#### Feature Standardisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNfmq_NzWNS8",
        "outputId": "4e7416fd-e600-4d4e-ee00-a286f6bfb59f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====== Feature Standardisation Started! ======\n",
            "\n",
            "====== Feature Standardisation Completed! ======\n",
            "The Shape for Training Set after Feature Standardisation: (20736, 1764)\n",
            "The Shape for Testing Set after Feature Standardisation: (1296, 1764)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n====== Feature Standardisation Started! ======\")\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "\n",
        "x_train_scaled = scaler.transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "print(\"\\n====== Feature Standardisation Completed! ======\")\n",
        "print(f\"The Shape for Training Set after Feature Standardisation: {x_train_scaled.shape}\")\n",
        "print(f\"The Shape for Testing Set after Feature Standardisation: {x_test_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ymXOdyBWNS8",
        "outputId": "2e379d3d-397a-41f6-9297-9439f30ca3f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====== Dimensionality Reduction by PCA Started! ======\n",
            "\n",
            "====== Dimensionality Reduction by PCA Completed! ======\n",
            "The Shape for Training Set after Dimensionality Reduction by PCA: (20736, 241)\n",
            "The Shape for Testing Set after Dimensionality Reduction by PCA: (1296, 241)\n",
            "The Number of Chosen PCA: 241\n",
            "The Explained Variance Ratio: 0.8505\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n====== Dimensionality Reduction by PCA Started! ======\")\n",
        "pca = PCA(n_components=0.85, random_state=42)\n",
        "pca.fit(x_train_scaled)\n",
        "\n",
        "\n",
        "x_train_pca = pca.transform(x_train_scaled)\n",
        "x_test_pca = pca.transform(x_test_scaled)\n",
        "\n",
        "print(\"\\n====== Dimensionality Reduction by PCA Completed! ======\")\n",
        "print(f\"The Shape for Training Set after Dimensionality Reduction by PCA: {x_train_pca.shape}\")\n",
        "print(f\"The Shape for Testing Set after Dimensionality Reduction by PCA: {x_test_pca.shape}\")\n",
        "print(f\"The Number of Chosen PCA: {pca.n_components_}\")\n",
        "print(f\"The Explained Variance Ratio: {np.sum(pca.explained_variance_ratio_):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnemcfUCWNS8"
      },
      "source": [
        "#### 1) RandomForestClassifier - feature extraction by hog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puCX8yRsWNS8",
        "outputId": "4032e1da-3cdf-48e9-8dda-b45403bc1b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training skipped, importing model trained previously...\n",
            "Fitted 3 folds for each of 30 candidates, totalling 30 fits\n"
          ]
        }
      ],
      "source": [
        "skip_train = True\n",
        "\n",
        "if skip_train:\n",
        "    # Import previous model\n",
        "    print(\"Training skipped, importing model trained previously...\")\n",
        "    print(\"Fitted 3 folds for each of 30 candidates, totalling 30 fits\")\n",
        "    with open('model_bank/best_hog_rf_model.pkl', 'rb') as file:\n",
        "        best_hog_rf = pickle.load(file)\n",
        "\n",
        "else:\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Base model\n",
        "    base_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Hyperparameters\n",
        "    param_dist = {\n",
        "        'n_estimators': [50, 100, 150, 200],\n",
        "        'max_depth': [10, 20, 30, 40],\n",
        "        'max_features': ['sqrt', 'log2', 0.5, 0.8, 1.0]\n",
        "    }\n",
        "\n",
        "    # Randomized search tuning\n",
        "    random_search = RandomizedSearchCV(\n",
        "        base_model,\n",
        "        param_dist,\n",
        "        n_iter=10,\n",
        "        scoring='accuracy',\n",
        "        cv=3,\n",
        "        verbose=2,\n",
        "        random_state=42,\n",
        "        error_score='raise',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(x_train_pca, y_train)\n",
        "\n",
        "    # End timing\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4P6lls7WNS9",
        "outputId": "682ad095-03db-4c8e-ab8e-79de2b31a265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training skipped, printing model trained previously...\n",
            "\n",
            "Best Parameters: {'n_estimators': 150, 'max_features': 0.5, 'max_depth': 40}\n",
            "Best Accuracy: 0.316551\n",
            "Total Training Time: 26.8 minutes\n"
          ]
        }
      ],
      "source": [
        "if skip_train:\n",
        "    print(\"Training skipped, printing model trained previously...\\n\")\n",
        "    best_hog_rf = random_search.best_estimator_\n",
        "    print(\"Best Parameters:\", random_search.best_params_)\n",
        "    print(f\"Best Accuracy: {random_search.best_score_:.6f}\")\n",
        "    training_time = 26.8\n",
        "else:\n",
        "    best_hog_rf = random_search.best_estimator_\n",
        "    print(\"Best Parameters:\", random_search.best_params_)\n",
        "    print(f\"Best Accuracy: {random_search.best_score_:.6f}\")\n",
        "    training_time = round(training_time /60, 2)\n",
        "print(f\"Total Training Time: {training_time} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrFZe45TWNS9",
        "outputId": "f91a3715-5e96-4dfd-f8d5-a14110d3ef64"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'best_hog_rf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m y_train_pred = \u001b[43mbest_hog_rf\u001b[49m.predict(x_train_pca)\n\u001b[32m      3\u001b[39m y_test_pred = best_hog_rf.predict(x_test_pca)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Function to save metrics\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'best_hog_rf' is not defined"
          ]
        }
      ],
      "source": [
        "# Predict\n",
        "y_train_pred = best_hog_rf.predict(x_train_pca)\n",
        "y_test_pred = best_hog_rf.predict(x_test_pca)\n",
        "\n",
        "# Function to save metrics\n",
        "metrics = {\"Metric\": [], \"Average Type\": [], \"Train\": [], \"Test\": []}\n",
        "def add_metric(name, avg_type, train_value, test_value):\n",
        "    metrics[\"Metric\"].append(name)\n",
        "    metrics[\"Average Type\"].append(avg_type)\n",
        "    metrics[\"Train\"].append(train_value)\n",
        "    metrics[\"Test\"].append(test_value)\n",
        "\n",
        "# Training time\n",
        "add_metric(\"Training time (minutes)\", \"N/A\", training_time, \"N/A\")\n",
        "\n",
        "# Accuracy\n",
        "add_metric(\"Accuracy\", \"N/A\",\n",
        "           accuracy_score(y_train, y_train_pred),\n",
        "           accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "# Precision\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(\"Precision\", avg,\n",
        "               precision_score(y_train, y_train_pred, average=avg),\n",
        "               precision_score(y_test, y_test_pred, average=avg))\n",
        "\n",
        "# Recall\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(\"Recall\", avg,\n",
        "               recall_score(y_train, y_train_pred, average=avg),\n",
        "               recall_score(y_test, y_test_pred, average=avg))\n",
        "\n",
        "# F0.5-Score\n",
        "beta = 0.5 # mis-labelled sneakers are more costly than missing labels\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(f\"F{beta}-Score\", avg,\n",
        "               fbeta_score(y_train, y_train_pred, beta=beta, average=avg),\n",
        "               fbeta_score(y_test, y_test_pred, beta=beta, average=avg))\n",
        "\n",
        "# Display metrics\n",
        "hog_rf_metrics = pd.DataFrame(metrics)\n",
        "pd.set_option('display.precision', 6)\n",
        "display(hog_rf_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFdW9eC1WNS9",
        "outputId": "74480ab5-27fa-49f5-8dc7-7757b54b53ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved Successfully ../CS610_AML_Group_Project/model_bank\\best_hog_rf_model.pkl\n"
          ]
        }
      ],
      "source": [
        "export = True\n",
        "\n",
        "if not export:\n",
        "    print(\"Model not exported\")\n",
        "else:\n",
        "    model_bank_dir = '../CS610_AML_Group_Project/model_bank'\n",
        "    os.makedirs(model_bank_dir, exist_ok=True)\n",
        "    model_filename_pickle = 'best_hog_rf_model.pkl'\n",
        "    model_path = os.path.join(model_bank_dir, model_filename_pickle)\n",
        "    with open(model_path, 'wb') as file:\n",
        "        pickle.dump(best_hog_rf, file)\n",
        "    print(f\"Model Saved Successfully {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqHv-h3KWNS-"
      },
      "source": [
        "#### 2) KNNClassifier - feature extraction by hog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe1r3kg-WNS-",
        "outputId": "1c4dace8-1713-415f-8b4c-50aadbd425c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        }
      ],
      "source": [
        "skip_train = False\n",
        "\n",
        "if skip_train:\n",
        "    # Import previous model\n",
        "    print(\"Training skipped, importing model trained previously...\")\n",
        "    print(\"Fitted 3 folds for each of 10 candidates, totalling 30 fits\")\n",
        "    with open('model_bank/best_hog_knn_model.pkl', 'rb') as file:\n",
        "        best_hog_knn = pickle.load(file)\n",
        "\n",
        "else:\n",
        "\n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Base model\n",
        "    base_model = KNeighborsClassifier()\n",
        "\n",
        "    # Hyperparameters\n",
        "    param_dist = {\n",
        "        'n_neighbors': randint(1, 30),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan', 'cosine']\n",
        "    }\n",
        "\n",
        "    # Randomized search tuning\n",
        "    random_search = RandomizedSearchCV(\n",
        "        base_model,\n",
        "        param_dist,\n",
        "        n_iter=10,\n",
        "        scoring='accuracy',\n",
        "        cv=3,\n",
        "        verbose=2,\n",
        "        random_state=42,\n",
        "        error_score='raise',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(x_train_pca, y_train)\n",
        "\n",
        "    # End timing\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFsc_BCDWNS-",
        "outputId": "1c5ee450-f8ed-4d75-9390-f9912e4d87cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'metric': 'euclidean', 'n_neighbors': 1, 'weights': 'distance'}\n",
            "Best Accuracy: 0.279900\n",
            "Total Training Time: 0.0 minutes\n"
          ]
        }
      ],
      "source": [
        "skip_train = False\n",
        "if skip_train:\n",
        "    print(\"Training skipped, printing model trained previously...\\n\")\n",
        "    print(\"Best Parameters: {'metric': 'euclidean', 'n_neighbors': 1, 'weights': 'distance'}\")\n",
        "    print(\"Best Accuracy: 0.675010\")\n",
        "    training_time = 1.03\n",
        "else:\n",
        "    best_hog_knn = random_search.best_estimator_\n",
        "    print(\"Best Parameters:\", random_search.best_params_)\n",
        "    print(f\"Best Accuracy: {random_search.best_score_:.6f}\")\n",
        "    training_time = round(training_time / 60, 2)\n",
        "print(f\"Total Training Time: {training_time} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZx6xOsOWNS-",
        "outputId": "bd01708e-f2be-45f8-9b73-8e4ab7165336"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Average Type</th>\n",
              "      <th>Train</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Training time (minutes)</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Accuracy</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.998939</td>\n",
              "      <td>0.368827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Precision</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.998861</td>\n",
              "      <td>0.379593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Precision</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.998939</td>\n",
              "      <td>0.368827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Precision</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.998971</td>\n",
              "      <td>0.385752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Recall</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.998801</td>\n",
              "      <td>0.367517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Recall</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.998939</td>\n",
              "      <td>0.368827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Recall</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.998939</td>\n",
              "      <td>0.368827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.998841</td>\n",
              "      <td>0.371319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.998939</td>\n",
              "      <td>0.368827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.998958</td>\n",
              "      <td>0.37661</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Metric Average Type     Train      Test\n",
              "0   Training time (minutes)          N/A  0.000000       N/A\n",
              "1                  Accuracy          N/A  0.998939  0.368827\n",
              "2                 Precision        macro  0.998861  0.379593\n",
              "3                 Precision        micro  0.998939  0.368827\n",
              "4                 Precision     weighted  0.998971  0.385752\n",
              "5                    Recall        macro  0.998801  0.367517\n",
              "6                    Recall        micro  0.998939  0.368827\n",
              "7                    Recall     weighted  0.998939  0.368827\n",
              "8                F0.5-Score        macro  0.998841  0.371319\n",
              "9                F0.5-Score        micro  0.998939  0.368827\n",
              "10               F0.5-Score     weighted  0.998958   0.37661"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Predict\n",
        "y_train_pred = best_hog_knn.predict(x_train_pca)\n",
        "y_test_pred = best_hog_knn.predict(x_test_pca)\n",
        "\n",
        "# Function to save metrics\n",
        "metrics = {\"Metric\": [], \"Average Type\": [], \"Train\": [], \"Test\": []}\n",
        "def add_metric(name, avg_type, train_value, test_value):\n",
        "    metrics[\"Metric\"].append(name)\n",
        "    metrics[\"Average Type\"].append(avg_type)\n",
        "    metrics[\"Train\"].append(train_value)\n",
        "    metrics[\"Test\"].append(test_value)\n",
        "\n",
        "# Training time\n",
        "add_metric(\"Training time (minutes)\", \"N/A\", training_time, \"N/A\")\n",
        "\n",
        "# Accuracy\n",
        "add_metric(\"Accuracy\", \"N/A\",\n",
        "           accuracy_score(y_train, y_train_pred),\n",
        "           accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "# Precision\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(\"Precision\", avg,\n",
        "               precision_score(y_train, y_train_pred, average=avg),\n",
        "               precision_score(y_test, y_test_pred, average=avg))\n",
        "\n",
        "# Recall\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(\"Recall\", avg,\n",
        "               recall_score(y_train, y_train_pred, average=avg),\n",
        "               recall_score(y_test, y_test_pred, average=avg))\n",
        "\n",
        "# F0.5-Score\n",
        "beta = 0.5 # mis-labelled sneakers are more costly than missing labels\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(f\"F{beta}-Score\", avg,\n",
        "               fbeta_score(y_train, y_train_pred, beta=beta, average=avg),\n",
        "               fbeta_score(y_test, y_test_pred, beta=beta, average=avg))\n",
        "\n",
        "# Display metrics\n",
        "hog_knn_metrics = pd.DataFrame(metrics)\n",
        "pd.set_option('display.precision', 6)\n",
        "display(hog_knn_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGUK16w8WNTF",
        "outputId": "128611e4-ad40-4190-a3c6-5ee2c5aefbf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved Successfully ../CS610_AML_Group_Project/model_bank\\best_hog_knn_model.pkl\n"
          ]
        }
      ],
      "source": [
        "export = True\n",
        "\n",
        "if not export:\n",
        "    print(\"Model not exported\")\n",
        "else:\n",
        "    model_bank_dir = '../CS610_AML_Group_Project/model_bank'\n",
        "    os.makedirs(model_bank_dir, exist_ok=True)\n",
        "    model_filename_pickle = 'best_hog_knn_model.pkl'\n",
        "    model_path = os.path.join(model_bank_dir, model_filename_pickle)\n",
        "    with open(model_path, 'wb') as file:\n",
        "        pickle.dump(best_hog_knn, file)\n",
        "    print(f\"Model Saved Successfully {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NgwMDpUWNTF"
      },
      "source": [
        "#### 3) XGBoostClassifier - feature extraction by hog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2Ig273yWNTF",
        "outputId": "3d4b4bca-68c6-4567-a820-c65a61c73993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        }
      ],
      "source": [
        "skip_train = False\n",
        "\n",
        "if skip_train:\n",
        "    # Import previous model\n",
        "    print(\"Training skipped, importing model trained previously...\")\n",
        "    print(\"Fitted 3 folds for each of 10 candidates, totalling 30 fits\")\n",
        "    with open('model_bank/best_hog_xgb_model.pkl', 'rb') as file:\n",
        "        best_hog_xgb = pickle.load(file)\n",
        "\n",
        "else:\n",
        "\n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Balance class weights\n",
        "    sample_weights = compute_sample_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        y=y_train\n",
        "    )\n",
        "\n",
        "    # Base model\n",
        "    base_model = xgb.XGBClassifier(\n",
        "        device=\"cuda\",\n",
        "        tree_method=\"hist\",\n",
        "        objective=\"multi:softprob\",\n",
        "        num_class=len(np.unique(y_train)),\n",
        "        eval_metric=['merror','mlogloss'],\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    # Hyperparameters\n",
        "    param_dist = {\n",
        "        'n_estimators': randint(50, 500),\n",
        "        'max_depth': randint(3, 12),\n",
        "        'learning_rate': uniform(0.01, 0.19),  # range: 0.01 to 0.2\n",
        "        'subsample': uniform(0.7, 0.3),        # range: 0.7 to 1.0\n",
        "        'colsample_bytree': uniform(0.7, 0.3)  # range: 0.7 to 1.0\n",
        "    }\n",
        "\n",
        "    # Randomized search tuning\n",
        "    random_search = RandomizedSearchCV(\n",
        "        base_model,\n",
        "        param_dist,\n",
        "        n_iter=10,\n",
        "        scoring='accuracy',\n",
        "        cv=3,\n",
        "        verbose=2,\n",
        "        random_state=42,\n",
        "        error_score='raise',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(\n",
        "        x_train_pca, y_train,\n",
        "        sample_weight = sample_weights)\n",
        "\n",
        "    # End timing\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cCa9eH0WNTF",
        "outputId": "30f657a5-b887-49f4-a0af-bdf65e8fde40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'colsample_bytree': 0.7692681476866446, 'learning_rate': 0.05579483854494223, 'max_depth': 9, 'n_estimators': 477, 'subsample': 0.848553073033381}\n",
            "Best Accuracy: 0.256028\n",
            "Total Training Time: 48.2 minutes\n"
          ]
        }
      ],
      "source": [
        "if skip_train:\n",
        "    print(\"Training skipped, printing model trained previously...\\n\")\n",
        "    print(\"Best Parameters: {'colsample_bytree': 0.7692681476866446, 'learning_rate': 0.05579483854494223, 'max_depth': 9, 'n_estimators': 477, 'subsample': 0.848553073033381}\")\n",
        "    print(\"Best Accuracy: 0.569396\")\n",
        "    training_time = 42.59\n",
        "else:\n",
        "    best_hog_xgb = random_search.best_estimator_\n",
        "    print(\"Best Parameters:\", random_search.best_params_)\n",
        "    print(f\"Best Accuracy: {random_search.best_score_:.6f}\")\n",
        "    training_time = round(training_time / 60, 2)\n",
        "print(f\"Total Training Time: {training_time} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NogAgSelWNTF",
        "outputId": "89f33ad2-aac8-4048-a962-262eff0b54b3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Average Type</th>\n",
              "      <th>Train</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Training time (minutes)</td>\n",
              "      <td>N/A</td>\n",
              "      <td>48.200000</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Accuracy</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.999035</td>\n",
              "      <td>0.341821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Precision</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.998918</td>\n",
              "      <td>0.353889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Precision</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.999035</td>\n",
              "      <td>0.341821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Precision</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.999046</td>\n",
              "      <td>0.353453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Recall</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.998934</td>\n",
              "      <td>0.337784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Recall</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.999035</td>\n",
              "      <td>0.341821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Recall</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.999035</td>\n",
              "      <td>0.341821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.998919</td>\n",
              "      <td>0.345981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.999035</td>\n",
              "      <td>0.341821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.999042</td>\n",
              "      <td>0.346817</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Metric Average Type      Train      Test\n",
              "0   Training time (minutes)          N/A  48.200000       N/A\n",
              "1                  Accuracy          N/A   0.999035  0.341821\n",
              "2                 Precision        macro   0.998918  0.353889\n",
              "3                 Precision        micro   0.999035  0.341821\n",
              "4                 Precision     weighted   0.999046  0.353453\n",
              "5                    Recall        macro   0.998934  0.337784\n",
              "6                    Recall        micro   0.999035  0.341821\n",
              "7                    Recall     weighted   0.999035  0.341821\n",
              "8                F0.5-Score        macro   0.998919  0.345981\n",
              "9                F0.5-Score        micro   0.999035  0.341821\n",
              "10               F0.5-Score     weighted   0.999042  0.346817"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Predict\n",
        "y_train_pred = best_hog_xgb.predict(x_train_pca)\n",
        "y_test_pred = best_hog_xgb.predict(x_test_pca)\n",
        "\n",
        "# Function to save metrics\n",
        "metrics = {\"Metric\": [], \"Average Type\": [], \"Train\": [], \"Test\": []}\n",
        "def add_metric(name, avg_type, train_value, test_value):\n",
        "    metrics[\"Metric\"].append(name)\n",
        "    metrics[\"Average Type\"].append(avg_type)\n",
        "    metrics[\"Train\"].append(train_value)\n",
        "    metrics[\"Test\"].append(test_value)\n",
        "\n",
        "# Training time\n",
        "add_metric(\"Training time (minutes)\", \"N/A\", training_time, \"N/A\")\n",
        "\n",
        "# Accuracy\n",
        "add_metric(\"Accuracy\", \"N/A\",\n",
        "           accuracy_score(y_train, y_train_pred),\n",
        "           accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "# Precision\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(\"Precision\", avg,\n",
        "               precision_score(y_train, y_train_pred, average=avg),\n",
        "               precision_score(y_test, y_test_pred, average=avg))\n",
        "\n",
        "# Recall\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(\"Recall\", avg,\n",
        "               recall_score(y_train, y_train_pred, average=avg),\n",
        "               recall_score(y_test, y_test_pred, average=avg))\n",
        "\n",
        "# F0.5-Score\n",
        "beta = 0.5 # mis-labelled sneakers are more costly than missing labels\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(f\"F{beta}-Score\", avg,\n",
        "               fbeta_score(y_train, y_train_pred, beta=beta, average=avg),\n",
        "               fbeta_score(y_test, y_test_pred, beta=beta, average=avg))\n",
        "\n",
        "# Display metrics\n",
        "hog_xgb_metrics = pd.DataFrame(metrics)\n",
        "pd.set_option('display.precision', 6)\n",
        "display(hog_xgb_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51oNSNwZWNTG",
        "outputId": "e2d74a2d-84fa-49c1-bec9-5c442f7595b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved Successfully ../CS610_AML_Group_Project/model_bank\\best_hog_xgb_model.pkl\n"
          ]
        }
      ],
      "source": [
        "export = True\n",
        "\n",
        "if not export:\n",
        "    print(\"Model not exported\")\n",
        "else:\n",
        "    model_bank_dir = '../CS610_AML_Group_Project/model_bank'\n",
        "    os.makedirs(model_bank_dir, exist_ok=True)\n",
        "    model_filename_pickle = 'best_hog_xgb_model.pkl'\n",
        "    model_path = os.path.join(model_bank_dir, model_filename_pickle)\n",
        "    with open(model_path, 'wb') as file:\n",
        "        pickle.dump(best_hog_xgb, file)\n",
        "    print(f\"Model Saved Successfully {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS1WWfEqWNTG"
      },
      "source": [
        "### Pipeline Models using Feature Extraction Method 2 - Using pretrained CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zpwse7UWNTG"
      },
      "source": [
        "ResNet50 will be used as the feature extractor due to its pre-trained weights, derived from large datasets like ImageNet, and is a popular choice to use for computer vision applications such as image classification.\n",
        "Reference:\n",
        "1) https://medium.com/@meetkalathiya1301/feature-extraction-using-pre-trained-models-for-image-classification-16e6ff43f268\n",
        "2) https://stackoverflow.com/questions/62117707/extract-features-from-pretrained-resnet50-in-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CJ43LufkWNTG"
      },
      "outputs": [],
      "source": [
        "#Process image data for feature extraction using CNN\n",
        "input_dir = '/content/CS610_AML_Group_Project/resized_images'\n",
        "img_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])]) #mean and std based on ImageNet - normalise image data closer to normal distribution\n",
        "img_dataset = datasets.ImageFolder(input_dir, transform=img_transform)\n",
        "data_loader = DataLoader(img_dataset, batch_size=32, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2mIQyFg6WNTG"
      },
      "outputs": [],
      "source": [
        "#define function for CNN feature extraction\n",
        "def cnn_feature_extract(cnn_feature_extractor, data_loader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    #prepare cnn model to use for feature extraction\n",
        "    cnn_feature_extractor.eval()\n",
        "    cnn_feature_extractor.fc = torch.nn.Identity() #replace fully connected layer of pretrained cnn with Identity layer\n",
        "    for para in cnn_feature_extractor.parameters():\n",
        "        para.requires_grad = False #freeze weights\n",
        "    #feature extraction\n",
        "    features_list, labels_list = [], []\n",
        "    cnn_feature_extractor.to(device)\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            feature = cnn_feature_extractor(images)\n",
        "            feature = feature.view(feature.size(0),-1) #flatten into (n_samples, n_features) for non-CNN models\n",
        "            #convert tensors into numpy for fitting into non-CNN models and add into lists\n",
        "            features_list.append(feature.cpu().numpy())\n",
        "            labels_list.append(labels.numpy())\n",
        "\n",
        "    return cnn_feature_extractor, np.vstack(features_list), np.hstack(labels_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z98FeozsWNTG",
        "outputId": "79d41c2d-7d0e-4852-92d3-2be638342535"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 215MB/s]\n"
          ]
        }
      ],
      "source": [
        "#initialise and extract features using CNN feature extractor\n",
        "weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
        "resnet50_extractor = models.resnet50(weights=weights)\n",
        "resnet50_extractor, X, y = cnn_feature_extract(resnet50_extractor, data_loader) #X = features, y =labels\n",
        "#no need labelling as the numpy array is generated from the data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWy22HFJWNTG",
        "outputId": "ea16b6ff-a658-4c08-8877-585ab0fa27b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Samples: 5184\n",
            "Number of Labels: 50\n",
            "Label Distribution:\n",
            "37    120\n",
            "16    120\n",
            "7     120\n",
            "13    120\n",
            "26    120\n",
            "36    120\n",
            "15    120\n",
            "18    120\n",
            "29    120\n",
            "0     120\n",
            "45    119\n",
            "2     119\n",
            "28    119\n",
            "33    119\n",
            "42    118\n",
            "20    118\n",
            "10    118\n",
            "41    118\n",
            "39    118\n",
            "5     118\n",
            "21    118\n",
            "48    118\n",
            "43    118\n",
            "49    116\n",
            "31     92\n",
            "1      92\n",
            "38     92\n",
            "4      92\n",
            "9      92\n",
            "35     92\n",
            "24     92\n",
            "30     92\n",
            "46     92\n",
            "40     92\n",
            "3      92\n",
            "19     92\n",
            "32     92\n",
            "44     91\n",
            "6      91\n",
            "11     91\n",
            "17     91\n",
            "12     91\n",
            "23     91\n",
            "22     90\n",
            "14     87\n",
            "47     87\n",
            "34     86\n",
            "27     85\n",
            "25     80\n",
            "8      73\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#CNN training and test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)\n",
        "x_train = pd.DataFrame(x_train, dtype=np.float32)\n",
        "y_train = pd.Series(y_train, dtype=np.int32)\n",
        "x_test = pd.DataFrame(x_test, dtype=np.float32)\n",
        "y_test = pd.Series(y_test, dtype=np.int32)\n",
        "#same as original flow\n",
        "print(\"Number of Samples:\", len(y_train))\n",
        "print(\"Number of Labels:\", len(np.unique(y_train)))\n",
        "counts = y_train.value_counts()\n",
        "print(\"Label Distribution:\")\n",
        "print(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqTPBSaTWNTG"
      },
      "source": [
        "#### 1) RandomForestClassifier - feature extraction by CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-Pfi2L4WNTG",
        "outputId": "e6858af1-0388-4218-bb2c-9e01da1a7244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        }
      ],
      "source": [
        "skip_train = False\n",
        "\n",
        "if skip_train:\n",
        "    # Import previous model\n",
        "    print(\"Training skipped, importing model trained previously...\")\n",
        "    print(\"Fitted 3 folds for each of 30 candidates, totalling 30 fits\")\n",
        "    with open('model_bank/best_cnn_rf_model.pkl', 'rb') as file:\n",
        "        best_cnn_rf = pickle.load(file)\n",
        "\n",
        "else:\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Base model\n",
        "    base_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Hyperparameters\n",
        "    param_dist = {\n",
        "        'n_estimators': [50, 100, 150, 200],\n",
        "        'max_depth': [10, 20, 30, 40],\n",
        "        'max_features': ['sqrt', 'log2', 0.5, 0.8, 1.0]\n",
        "    }\n",
        "\n",
        "    # Randomized search tuning\n",
        "    random_search = RandomizedSearchCV(\n",
        "        base_model,\n",
        "        param_dist,\n",
        "        n_iter=10,\n",
        "        scoring='accuracy',\n",
        "        cv=3,\n",
        "        verbose=2,\n",
        "        random_state=42,\n",
        "        error_score='raise',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(x_train, y_train)\n",
        "\n",
        "    # End timing\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yipipkk2WNTH",
        "outputId": "c264eb7f-d67a-4e66-901e-c80d582eafb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'n_estimators': 150, 'max_features': 0.5, 'max_depth': 40}\n",
            "Best Accuracy: 0.316551\n",
            "Total Training Time: 49.15 minutes\n"
          ]
        }
      ],
      "source": [
        "if skip_train:\n",
        "    print(\"Training skipped, printing model trained previously...\\n\")\n",
        "    print(\"Best Parameters: {'n_estimators': 150, 'max_features': 'sqrt', 'max_depth': 20}\")\n",
        "    print(\"Best Accuracy: 0.719473\")\n",
        "    training_time = 87.56\n",
        "else:\n",
        "    best_cnn_rf = random_search.best_estimator_\n",
        "    print(\"Best Parameters:\", random_search.best_params_)\n",
        "    print(f\"Best Accuracy: {random_search.best_score_:.6f}\")\n",
        "    training_time = round(training_time / 60, 2)\n",
        "print(f\"Total Training Time: {training_time} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFBjVgFQWNTH",
        "outputId": "558078c4-169f-4541-ada4-66697b003645"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Average Type</th>\n",
              "      <th>Train</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Training time (minutes)</td>\n",
              "      <td>N/A</td>\n",
              "      <td>49.150000</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Accuracy</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.368827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Precision</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.997978</td>\n",
              "      <td>0.391356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Precision</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.368827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Precision</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.998166</td>\n",
              "      <td>0.38563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Recall</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.997817</td>\n",
              "      <td>0.357844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Recall</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.368827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Recall</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.368827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.997926</td>\n",
              "      <td>0.356892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.368827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.998129</td>\n",
              "      <td>0.357856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Metric Average Type      Train      Test\n",
              "0   Training time (minutes)          N/A  49.150000       N/A\n",
              "1                  Accuracy          N/A   0.998071  0.368827\n",
              "2                 Precision        macro   0.997978  0.391356\n",
              "3                 Precision        micro   0.998071  0.368827\n",
              "4                 Precision     weighted   0.998166   0.38563\n",
              "5                    Recall        macro   0.997817  0.357844\n",
              "6                    Recall        micro   0.998071  0.368827\n",
              "7                    Recall     weighted   0.998071  0.368827\n",
              "8                F0.5-Score        macro   0.997926  0.356892\n",
              "9                F0.5-Score        micro   0.998071  0.368827\n",
              "10               F0.5-Score     weighted   0.998129  0.357856"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Predict\n",
        "y_train_pred = best_cnn_rf.predict(x_train)\n",
        "y_test_pred = best_cnn_rf.predict(x_test)\n",
        "\n",
        "# Function to save metrics\n",
        "metrics = {\"Metric\": [], \"Average Type\": [], \"Train\": [], \"Test\": []}\n",
        "def add_metric(name, avg_type, train_value, test_value):\n",
        "    metrics[\"Metric\"].append(name)\n",
        "    metrics[\"Average Type\"].append(avg_type)\n",
        "    metrics[\"Train\"].append(train_value)\n",
        "    metrics[\"Test\"].append(test_value)\n",
        "\n",
        "# Training time\n",
        "add_metric(\"Training time (minutes)\", \"N/A\", training_time, \"N/A\")\n",
        "\n",
        "# Accuracy\n",
        "add_metric(\"Accuracy\", \"N/A\",\n",
        "           accuracy_score(y_train, y_train_pred),\n",
        "           accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "# Precision\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(\"Precision\", avg,\n",
        "               precision_score(y_train, y_train_pred, average=avg),\n",
        "               precision_score(y_test, y_test_pred, average=avg))\n",
        "\n",
        "# Recall\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(\"Recall\", avg,\n",
        "               recall_score(y_train, y_train_pred, average=avg),\n",
        "               recall_score(y_test, y_test_pred, average=avg))\n",
        "\n",
        "# F0.5-Score\n",
        "beta = 0.5 # mis-labelled sneakers are more costly than missing labels\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(f\"F{beta}-Score\", avg,\n",
        "               fbeta_score(y_train, y_train_pred, beta=beta, average=avg),\n",
        "               fbeta_score(y_test, y_test_pred, beta=beta, average=avg))\n",
        "\n",
        "# Display metrics\n",
        "cnn_rf_metrics = pd.DataFrame(metrics)\n",
        "pd.set_option('display.precision', 6)\n",
        "display(cnn_rf_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL8p5b-uWNTH",
        "outputId": "2d1a203d-379f-449d-b659-819fd9e10e8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved Successfully ../CS610_AML_Group_Project/model_bank\\best_cnn_rf_model.pkl\n"
          ]
        }
      ],
      "source": [
        "export = True\n",
        "\n",
        "if not export:\n",
        "    print(\"Model not exported\")\n",
        "else:\n",
        "    model_bank_dir = '../CS610_AML_Group_Project/model_bank'\n",
        "    os.makedirs(model_bank_dir, exist_ok=True)\n",
        "    model_filename_pickle = 'best_cnn_rf_model.pkl'\n",
        "    model_path = os.path.join(model_bank_dir, model_filename_pickle)\n",
        "    with open(model_path, 'wb') as file:\n",
        "        pickle.dump(best_cnn_rf, file)\n",
        "    print(f\"Model Saved Successfully {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_znyl5xFWNTH"
      },
      "source": [
        "#### 2) KNNClassifier - feature extraction by CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUt8Ges0WNTH",
        "outputId": "745c2cd2-e69c-4526-bb1c-3068c0bdff1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        }
      ],
      "source": [
        "skip_train = False\n",
        "\n",
        "if skip_train:\n",
        "    # Import previous model\n",
        "    print(\"Training skipped, importing model trained previously...\")\n",
        "    print(\"Fitted 3 folds for each of 10 candidates, totalling 30 fits\")\n",
        "    with open('model_bank/best_cnn_knn_model.pkl', 'rb') as file:\n",
        "        best_cnn_knn = pickle.load(file)\n",
        "\n",
        "else:\n",
        "\n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Base model\n",
        "    base_model = KNeighborsClassifier()\n",
        "\n",
        "    # Hyperparameters\n",
        "    param_dist = {\n",
        "        'n_neighbors': randint(1, 30),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan', 'cosine']\n",
        "    }\n",
        "\n",
        "    # Randomized search tuning\n",
        "    random_search = RandomizedSearchCV(\n",
        "        base_model,\n",
        "        param_dist,\n",
        "        n_iter=10,\n",
        "        scoring='accuracy',\n",
        "        cv=3,\n",
        "        verbose=2,\n",
        "        random_state=42,\n",
        "        error_score='raise',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(x_train, y_train)\n",
        "\n",
        "    # End timing\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTYhxWHwWNTH",
        "outputId": "27d356ab-f57d-4740-dcfd-87e7ee53ccc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'metric': 'cosine', 'n_neighbors': 11, 'weights': 'distance'}\n",
            "Best Accuracy: 0.302083\n",
            "Total Training Time: 0.22 minutes\n"
          ]
        }
      ],
      "source": [
        "if skip_train:\n",
        "    print(\"Training skipped, printing model trained previously...\\n\")\n",
        "    print(\"Best Parameters: {'metric': 'euclidean', 'n_neighbors': 1, 'weights': 'distance'}\")\n",
        "    print(\"Best Accuracy: 0.896123\")\n",
        "    training_time = 3.96\n",
        "else:\n",
        "    best_cnn_knn = random_search.best_estimator_\n",
        "    print(\"Best Parameters:\", random_search.best_params_)\n",
        "    print(f\"Best Accuracy: {random_search.best_score_:.6f}\")\n",
        "    training_time = round(training_time / 60, 2)\n",
        "print(f\"Total Training Time: {training_time} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8R8cbx7WNTH",
        "outputId": "ab8797a7-2c83-4849-8c45-cf31613e58e8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Average Type</th>\n",
              "      <th>Train</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Training time (minutes)</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Accuracy</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.367284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Precision</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.997988</td>\n",
              "      <td>0.389494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Precision</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.367284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Precision</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.998176</td>\n",
              "      <td>0.393255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Recall</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.997819</td>\n",
              "      <td>0.361929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Recall</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.367284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Recall</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.367284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.997929</td>\n",
              "      <td>0.37711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.367284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.998133</td>\n",
              "      <td>0.381005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Metric Average Type     Train      Test\n",
              "0   Training time (minutes)          N/A  0.220000       N/A\n",
              "1                  Accuracy          N/A  0.998071  0.367284\n",
              "2                 Precision        macro  0.997988  0.389494\n",
              "3                 Precision        micro  0.998071  0.367284\n",
              "4                 Precision     weighted  0.998176  0.393255\n",
              "5                    Recall        macro  0.997819  0.361929\n",
              "6                    Recall        micro  0.998071  0.367284\n",
              "7                    Recall     weighted  0.998071  0.367284\n",
              "8                F0.5-Score        macro  0.997929   0.37711\n",
              "9                F0.5-Score        micro  0.998071  0.367284\n",
              "10               F0.5-Score     weighted  0.998133  0.381005"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Predict\n",
        "y_train_pred = best_cnn_knn.predict(x_train)\n",
        "y_test_pred = best_cnn_knn.predict(x_test)\n",
        "\n",
        "# Function to save metrics\n",
        "metrics = {\"Metric\": [], \"Average Type\": [], \"Train\": [], \"Test\": []}\n",
        "def add_metric(name, avg_type, train_value, test_value):\n",
        "    metrics[\"Metric\"].append(name)\n",
        "    metrics[\"Average Type\"].append(avg_type)\n",
        "    metrics[\"Train\"].append(train_value)\n",
        "    metrics[\"Test\"].append(test_value)\n",
        "\n",
        "# Training time\n",
        "add_metric(\"Training time (minutes)\", \"N/A\", training_time, \"N/A\")\n",
        "\n",
        "# Accuracy\n",
        "add_metric(\"Accuracy\", \"N/A\",\n",
        "           accuracy_score(y_train, y_train_pred),\n",
        "           accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "# Precision\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(\"Precision\", avg,\n",
        "               precision_score(y_train, y_train_pred, average=avg),\n",
        "               precision_score(y_test, y_test_pred, average=avg))\n",
        "\n",
        "# Recall\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(\"Recall\", avg,\n",
        "               recall_score(y_train, y_train_pred, average=avg),\n",
        "               recall_score(y_test, y_test_pred, average=avg))\n",
        "\n",
        "# F0.5-Score\n",
        "beta = 0.5 # mis-labelled sneakers are more costly than missing labels\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(f\"F{beta}-Score\", avg,\n",
        "               fbeta_score(y_train, y_train_pred, beta=beta, average=avg),\n",
        "               fbeta_score(y_test, y_test_pred, beta=beta, average=avg))\n",
        "\n",
        "# Display metrics\n",
        "cnn_knn_metrics = pd.DataFrame(metrics)\n",
        "pd.set_option('display.precision', 6)\n",
        "display(cnn_knn_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG5bQkxMWNTI",
        "outputId": "03931b54-7f26-49d8-fb7c-43acdc207606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved Successfully ../CS610_AML_Group_Project/model_bank\\best_cnn_knn_model.pkl\n"
          ]
        }
      ],
      "source": [
        "export = True\n",
        "\n",
        "if not export:\n",
        "    print(\"Model not exported\")\n",
        "else:\n",
        "    model_bank_dir = '../CS610_AML_Group_Project/model_bank'\n",
        "    os.makedirs(model_bank_dir, exist_ok=True)\n",
        "    model_filename_pickle = 'best_cnn_knn_model.pkl'\n",
        "    model_path = os.path.join(model_bank_dir, model_filename_pickle)\n",
        "    with open(model_path, 'wb') as file:\n",
        "        pickle.dump(best_cnn_knn, file)\n",
        "    print(f\"Model Saved Successfully {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pfwmSqFWNTJ"
      },
      "source": [
        "#### 3) XGBoostClassifier - feature extraction by CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7Wf7WozWNTJ",
        "outputId": "8825375e-5ffc-4c18-907e-c4c12c2f29ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        }
      ],
      "source": [
        "skip_train = False\n",
        "\n",
        "if skip_train:\n",
        "    # Import previous model\n",
        "    print(\"Training skipped, importing model trained previously...\")\n",
        "    print(\"Fitted 3 folds for each of 30 candidates, totalling 30 fits\")\n",
        "    with open('model_bank/best_cnn_xgb_model.pkl', 'rb') as file:\n",
        "        best_cnn_xgb = pickle.load(file)\n",
        "\n",
        "else:\n",
        "\n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Balance class weights\n",
        "    sample_weights = compute_sample_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        y=y_train\n",
        "    )\n",
        "\n",
        "    # Base model\n",
        "    base_model = xgb.XGBClassifier(\n",
        "        device=\"cuda\",\n",
        "        tree_method=\"hist\",\n",
        "        objective=\"multi:softprob\",\n",
        "        num_class=len(np.unique(y_train)),\n",
        "        eval_metric=['merror','mlogloss'],\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    # Hyperparameters\n",
        "    param_dist = {\n",
        "        'n_estimators': randint(50, 500),\n",
        "        'max_depth': randint(3, 12),\n",
        "        'learning_rate': uniform(0.01, 0.19),  # range: 0.01 to 0.2\n",
        "        'subsample': uniform(0.7, 0.3),        # range: 0.7 to 1.0\n",
        "        'colsample_bytree': uniform(0.7, 0.3)  # range: 0.7 to 1.0\n",
        "    }\n",
        "\n",
        "    # Randomized search tuning\n",
        "    random_search = RandomizedSearchCV(\n",
        "        base_model,\n",
        "        param_dist,\n",
        "        n_iter=10,\n",
        "        scoring='accuracy',\n",
        "        cv=3,\n",
        "        verbose=2,\n",
        "        random_state=42,\n",
        "        error_score='raise',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(\n",
        "        x_train, y_train,\n",
        "        sample_weight = sample_weights)\n",
        "\n",
        "    # End timing\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZcdh-ViWNTJ",
        "outputId": "669fbb72-70cc-44e9-a589-fa7cae30aedb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'colsample_bytree': np.float64(0.7692681476866446), 'learning_rate': np.float64(0.05579483854494223), 'max_depth': 9, 'n_estimators': 477, 'subsample': np.float64(0.848553073033381)}\n",
            "Best Accuracy: 0.410687\n",
            "Total Training Time: 0.79 minutes\n"
          ]
        }
      ],
      "source": [
        "if skip_train:\n",
        "    print(\"Training skipped, printing model trained previously...\\n\")\n",
        "    print(\"Best Parameters: {'colsample_bytree': 0.7195154778955838, 'learning_rate': 0.19028825207813332, 'max_depth': 4, 'n_estimators': 314, 'subsample': 0.7047898756660642}\")\n",
        "    print(\"Best Accuracy: 0.796345\")\n",
        "    training_time = 216.38\n",
        "else:\n",
        "    best_cnn_xgb = random_search.best_estimator_\n",
        "    print(\"Best Parameters:\", random_search.best_params_)\n",
        "    print(f\"Best Accuracy: {random_search.best_score_:.6f}\")\n",
        "    training_time = round(training_time / 60, 2)\n",
        "print(f\"Total Training Time: {training_time} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "93la-IuzWNTJ",
        "outputId": "ef9a6c52-801a-4632-981c-0b68d366ea2c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"cnn_xgb_metrics\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"Metric\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Accuracy\",\n          \"F0.5-Score\",\n          \"Precision\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Average Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"macro\",\n          \"weighted\",\n          \"N/A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06272785001356379,\n        \"min\": 0.79,\n        \"max\": 0.9982039883100218,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.79,\n          0.998070987654321,\n          0.9979085568600852\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Test\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"N/A\",\n          0.49151234567901236,\n          0.4872799881124996\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "cnn_xgb_metrics"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-4b6f366a-9fa2-4508-9c0c-769eb7a9e0fb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Average Type</th>\n",
              "      <th>Train</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Training time (minutes)</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Accuracy</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.491512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Precision</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.997954</td>\n",
              "      <td>0.493303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Precision</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.491512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Precision</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.998204</td>\n",
              "      <td>0.494231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Recall</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.997872</td>\n",
              "      <td>0.486433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Recall</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.491512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Recall</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.491512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>macro</td>\n",
              "      <td>0.997909</td>\n",
              "      <td>0.48728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>micro</td>\n",
              "      <td>0.998071</td>\n",
              "      <td>0.491512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>F0.5-Score</td>\n",
              "      <td>weighted</td>\n",
              "      <td>0.998152</td>\n",
              "      <td>0.489526</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b6f366a-9fa2-4508-9c0c-769eb7a9e0fb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4b6f366a-9fa2-4508-9c0c-769eb7a9e0fb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4b6f366a-9fa2-4508-9c0c-769eb7a9e0fb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-044badd1-0805-4e2e-a229-f6cc245e72ea\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-044badd1-0805-4e2e-a229-f6cc245e72ea')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-044badd1-0805-4e2e-a229-f6cc245e72ea button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_9713f99f-b7b0-4b67-96d4-df70924b06f8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('cnn_xgb_metrics')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9713f99f-b7b0-4b67-96d4-df70924b06f8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('cnn_xgb_metrics');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                     Metric Average Type     Train      Test\n",
              "0   Training time (minutes)          N/A  0.790000       N/A\n",
              "1                  Accuracy          N/A  0.998071  0.491512\n",
              "2                 Precision        macro  0.997954  0.493303\n",
              "3                 Precision        micro  0.998071  0.491512\n",
              "4                 Precision     weighted  0.998204  0.494231\n",
              "5                    Recall        macro  0.997872  0.486433\n",
              "6                    Recall        micro  0.998071  0.491512\n",
              "7                    Recall     weighted  0.998071  0.491512\n",
              "8                F0.5-Score        macro  0.997909   0.48728\n",
              "9                F0.5-Score        micro  0.998071  0.491512\n",
              "10               F0.5-Score     weighted  0.998152  0.489526"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Predict\n",
        "y_train_pred = best_cnn_xgb.predict(x_train)\n",
        "y_test_pred = best_cnn_xgb.predict(x_test)\n",
        "\n",
        "# Function to save metrics\n",
        "metrics = {\"Metric\": [], \"Average Type\": [], \"Train\": [], \"Test\": []}\n",
        "def add_metric(name, avg_type, train_value, test_value):\n",
        "    metrics[\"Metric\"].append(name)\n",
        "    metrics[\"Average Type\"].append(avg_type)\n",
        "    metrics[\"Train\"].append(train_value)\n",
        "    metrics[\"Test\"].append(test_value)\n",
        "\n",
        "# Training time\n",
        "add_metric(\"Training time (minutes)\", \"N/A\", training_time, \"N/A\")\n",
        "\n",
        "# Accuracy\n",
        "add_metric(\"Accuracy\", \"N/A\",\n",
        "           accuracy_score(y_train, y_train_pred),\n",
        "           accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "# Precision\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(\"Precision\", avg,\n",
        "               precision_score(y_train, y_train_pred, average=avg),\n",
        "               precision_score(y_test, y_test_pred, average=avg))\n",
        "\n",
        "# Recall\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(\"Recall\", avg,\n",
        "               recall_score(y_train, y_train_pred, average=avg),\n",
        "               recall_score(y_test, y_test_pred, average=avg))\n",
        "\n",
        "# F0.5-Score\n",
        "beta = 0.5 # mis-labelled sneakers are more costly than missing labels\n",
        "for avg in ['macro', 'micro', 'weighted']:\n",
        "    add_metric(f\"F{beta}-Score\", avg,\n",
        "               fbeta_score(y_train, y_train_pred, beta=beta, average=avg),\n",
        "               fbeta_score(y_test, y_test_pred, beta=beta, average=avg))\n",
        "\n",
        "# Display metrics\n",
        "cnn_xgb_metrics = pd.DataFrame(metrics)\n",
        "pd.set_option('display.precision', 6)\n",
        "display(cnn_xgb_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YbROYUbWNTJ",
        "outputId": "e30bbc03-e7b0-4cb7-806f-87a3b8200d56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved Successfully /content/CS610_AML_Group_Project/model_bank/best_cnn_xgb_model.pkl\n"
          ]
        }
      ],
      "source": [
        "export = True\n",
        "\n",
        "if not export:\n",
        "    print(\"Model not exported\")\n",
        "else:\n",
        "    model_bank_dir = '/content/CS610_AML_Group_Project/model_bank'\n",
        "    os.makedirs(model_bank_dir, exist_ok=True)\n",
        "    model_filename_pickle = 'best_cnn_xgb_model.pkl'\n",
        "    model_path = os.path.join(model_bank_dir, model_filename_pickle)\n",
        "    with open(model_path, 'wb') as file:\n",
        "        pickle.dump(best_cnn_xgb, file)\n",
        "    print(f\"Model Saved Successfully {model_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HnemcfUCWNS8",
        "GqHv-h3KWNS-"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
